[
  {
    "sentence": "Prior approaches extract one causal graph given long-time observations.",
    "subtitle": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 14,
    "para_id": 2,
    "paper_id": 0,
    "2d_coord": [
      -0.1098870187997818,
      -1.9116504192352295
    ],
    "MSU_id": 14,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Abstract— The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time- varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long- time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in- depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time- varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi- dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real- world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts."
  },
  {
    "sentence": "Prior approaches cannot be directly applied to capturing, interpreting, and validating dynamic urban causality.",
    "subtitle": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 15,
    "para_id": 2,
    "paper_id": 0,
    "2d_coord": [
      -1.7960368394851685,
      -1.9831196069717407
    ],
    "MSU_id": 15,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Abstract— The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time- varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long- time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in- depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time- varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi- dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real- world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts."
  },
  {
    "sentence": "The patterns and insights derived in these studies do not imply true causality.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 31,
    "para_id": 4,
    "paper_id": 0,
    "2d_coord": [
      -0.5977627038955688,
      -2.4384169578552246
    ],
    "MSU_id": 31,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Many efforts have been devoted to the efficient extraction and extensive analysis of the co- occurrence [15, 36, 75] and correlation relations [45, 63] among urban time series or events. However, the patterns and insights derived in these studies do not imply true causality, and thus the reliability of the results is often limited in applications. Recent studies [38, 89] have investigated applying the Granger causality test to capture the causal relations among urban time series. Based on a set of urban time series, these studies attempt to extract a causal graph, where each cause- effect relation between a pair of sensors is represented with a directed edge. However, one causal graph is insufficient to characterize the dynamic causal relations among multiple urban time series due to the rapidly- changing urban environments. For example, a causal relation between two sensors detected based on their time series can disappear or even be flipped from time to time (Fig. 2a) because of certain external factors, such as the wind fields in the air pollution scenarios. Such a dynamic nature demands fine- grained causality analyses that enable analysts to gain insights into the temporal variations of causal relations in urban contexts. In addition to the limitation of granularity in the prior studies, interpreting and validating these causal relations detected by automated models also require an interactive system to integrate analysts in the urban causality analysis loop [76]."
  },
  {
    "sentence": "The reliability of the results is often limited in applications.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 32,
    "para_id": 4,
    "paper_id": 0,
    "2d_coord": [
      -1.0701806545257568,
      -1.6586734056472778
    ],
    "MSU_id": 32,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Many efforts have been devoted to the efficient extraction and extensive analysis of the co- occurrence [15, 36, 75] and correlation relations [45, 63] among urban time series or events. However, the patterns and insights derived in these studies do not imply true causality, and thus the reliability of the results is often limited in applications. Recent studies [38, 89] have investigated applying the Granger causality test to capture the causal relations among urban time series. Based on a set of urban time series, these studies attempt to extract a causal graph, where each cause- effect relation between a pair of sensors is represented with a directed edge. However, one causal graph is insufficient to characterize the dynamic causal relations among multiple urban time series due to the rapidly- changing urban environments. For example, a causal relation between two sensors detected based on their time series can disappear or even be flipped from time to time (Fig. 2a) because of certain external factors, such as the wind fields in the air pollution scenarios. Such a dynamic nature demands fine- grained causality analyses that enable analysts to gain insights into the temporal variations of causal relations in urban contexts. In addition to the limitation of granularity in the prior studies, interpreting and validating these causal relations detected by automated models also require an interactive system to integrate analysts in the urban causality analysis loop [76]."
  },
  {
    "sentence": "In addition to the limitation of granularity in the prior studies, interpreting and validating these causal relations detected by automated models also require an interactive system.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 40,
    "para_id": 4,
    "paper_id": 0,
    "2d_coord": [
      -0.4174244999885559,
      -2.3003246784210205
    ],
    "MSU_id": 40,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Many efforts have been devoted to the efficient extraction and extensive analysis of the co- occurrence [15, 36, 75] and correlation relations [45, 63] among urban time series or events. However, the patterns and insights derived in these studies do not imply true causality, and thus the reliability of the results is often limited in applications. Recent studies [38, 89] have investigated applying the Granger causality test to capture the causal relations among urban time series. Based on a set of urban time series, these studies attempt to extract a causal graph, where each cause- effect relation between a pair of sensors is represented with a directed edge. However, one causal graph is insufficient to characterize the dynamic causal relations among multiple urban time series due to the rapidly- changing urban environments. For example, a causal relation between two sensors detected based on their time series can disappear or even be flipped from time to time (Fig. 2a) because of certain external factors, such as the wind fields in the air pollution scenarios. Such a dynamic nature demands fine- grained causality analyses that enable analysts to gain insights into the temporal variations of causal relations in urban contexts. In addition to the limitation of granularity in the prior studies, interpreting and validating these causal relations detected by automated models also require an interactive system to integrate analysts in the urban causality analysis loop [76]."
  },
  {
    "sentence": "Prior studies have limitations in retrieving and comprehending dynamic causal relations among urban time series.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 42,
    "para_id": 5,
    "paper_id": 0,
    "2d_coord": [
      -1.3655753135681152,
      -1.1966805458068848
    ],
    "MSU_id": 42,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Motivated by the limitations in the prior studies, we propose a visual analytics approach that empowers experts to retrieve and comprehend the dynamic causal relations among urban time series with tailored interactions and visualizations. Developing such an approach poses three major challenges:"
  },
  {
    "sentence": "Such experiments require stakeholders to test numerous variable pairs tediously.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 79,
    "para_id": 12,
    "paper_id": 0,
    "2d_coord": [
      -0.38849523663520813,
      -2.093467950820923
    ],
    "MSU_id": 79,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "The traditional causal analysis method is controlled variable experiments, e.g., A/B testings in business companies. However, such experiments require stakeholders to test numerous variable pairs tediously and identify the causal relations among variables. By contrast, causal detection is a data- driven approach to obtain the causal relations among observed data without setting experimental conditions deliberately."
  },
  {
    "sentence": "Such experiments require stakeholders to identify the causal relations among variables.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 80,
    "para_id": 12,
    "paper_id": 0,
    "2d_coord": [
      0.14382712543010712,
      -1.6973824501037598
    ],
    "MSU_id": 80,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "The traditional causal analysis method is controlled variable experiments, e.g., A/B testings in business companies. However, such experiments require stakeholders to test numerous variable pairs tediously and identify the causal relations among variables. By contrast, causal detection is a data- driven approach to obtain the causal relations among observed data without setting experimental conditions deliberately."
  },
  {
    "sentence": "Even if the result is correct, further interpretation and verification are required for informed policy making.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 88,
    "para_id": 13,
    "paper_id": 0,
    "2d_coord": [
      -1.801397681236267,
      -1.5850074291229248
    ],
    "MSU_id": 88,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Fig. 2. Motivation illustration. (a) Real-world causal relations between sensors can change over time, shown by the different relations in three time windows. (b, c) Causal detection in large time windows may produce wrong or rough results. (d) Although the result is correct, further interpretation and verification are still required for informed policy making."
  },
  {
    "sentence": "Jin et al. transformed event sequences into time series based on Hawkes processes and applied the Granger causality test to these series.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 101,
    "para_id": 15,
    "paper_id": 0,
    "2d_coord": [
      -1.7760862112045288,
      -1.5023362636566162
    ],
    "MSU_id": 101,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Granger causality is not limited to traditional time series. To detect the causal relations among events, Jin et al. [31] and Xu et al. [79] transformed event sequences into time series based on Hawkes processes [28] and applied the Granger causality test to these series. Some researchers extended the Granger causality test into urban time series that are associated with geographic positions. Li et al. [38] extracted upstream events from air pollution time series and constructed the causal graphs among these events based on the Granger causality test. Frequent subgraph mining was then applied to extract the propagation pattern of air pollution. Zhu et al. [89] integrated the Granger causality test into Bayesian learning to identify the causal networks of air pollution sensors. However, none of the existing methods can accurately detect the dynamic causality of the urban time series, let alone interpret and validate the causality."
  },
  {
    "sentence": "Xu et al. transformed event sequences into time series based on Hawkes processes and applied the Granger causality test to these series.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 102,
    "para_id": 15,
    "paper_id": 0,
    "2d_coord": [
      -2.315314769744873,
      -1.2385181188583374
    ],
    "MSU_id": 102,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Granger causality is not limited to traditional time series. To detect the causal relations among events, Jin et al. [31] and Xu et al. [79] transformed event sequences into time series based on Hawkes processes [28] and applied the Granger causality test to these series. Some researchers extended the Granger causality test into urban time series that are associated with geographic positions. Li et al. [38] extracted upstream events from air pollution time series and constructed the causal graphs among these events based on the Granger causality test. Frequent subgraph mining was then applied to extract the propagation pattern of air pollution. Zhu et al. [89] integrated the Granger causality test into Bayesian learning to identify the causal networks of air pollution sensors. However, none of the existing methods can accurately detect the dynamic causality of the urban time series, let alone interpret and validate the causality."
  },
  {
    "sentence": "Li et al. extracted upstream events from air pollution time series and constructed the causal graphs among these events based on the Granger causality test.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 104,
    "para_id": 15,
    "paper_id": 0,
    "2d_coord": [
      -2.1487278938293457,
      -1.3664425611495972
    ],
    "MSU_id": 104,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Granger causality is not limited to traditional time series. To detect the causal relations among events, Jin et al. [31] and Xu et al. [79] transformed event sequences into time series based on Hawkes processes [28] and applied the Granger causality test to these series. Some researchers extended the Granger causality test into urban time series that are associated with geographic positions. Li et al. [38] extracted upstream events from air pollution time series and constructed the causal graphs among these events based on the Granger causality test. Frequent subgraph mining was then applied to extract the propagation pattern of air pollution. Zhu et al. [89] integrated the Granger causality test into Bayesian learning to identify the causal networks of air pollution sensors. However, none of the existing methods can accurately detect the dynamic causality of the urban time series, let alone interpret and validate the causality."
  },
  {
    "sentence": "Frequent subgraph mining was applied to extract the propagation pattern of air pollution.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 105,
    "para_id": 15,
    "paper_id": 0,
    "2d_coord": [
      -0.4797677993774414,
      -2.4019360542297363
    ],
    "MSU_id": 105,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Granger causality is not limited to traditional time series. To detect the causal relations among events, Jin et al. [31] and Xu et al. [79] transformed event sequences into time series based on Hawkes processes [28] and applied the Granger causality test to these series. Some researchers extended the Granger causality test into urban time series that are associated with geographic positions. Li et al. [38] extracted upstream events from air pollution time series and constructed the causal graphs among these events based on the Granger causality test. Frequent subgraph mining was then applied to extract the propagation pattern of air pollution. Zhu et al. [89] integrated the Granger causality test into Bayesian learning to identify the causal networks of air pollution sensors. However, none of the existing methods can accurately detect the dynamic causality of the urban time series, let alone interpret and validate the causality."
  },
  {
    "sentence": "Zhu et al. integrated the Granger causality test into Bayesian learning to identify the causal networks of air pollution sensors.",
    "subtitle": "2.1 Causal Detection",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 106,
    "para_id": 15,
    "paper_id": 0,
    "2d_coord": [
      -2.1671881675720215,
      -1.8853352069854736
    ],
    "MSU_id": 106,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Granger causality is not limited to traditional time series. To detect the causal relations among events, Jin et al. [31] and Xu et al. [79] transformed event sequences into time series based on Hawkes processes [28] and applied the Granger causality test to these series. Some researchers extended the Granger causality test into urban time series that are associated with geographic positions. Li et al. [38] extracted upstream events from air pollution time series and constructed the causal graphs among these events based on the Granger causality test. Frequent subgraph mining was then applied to extract the propagation pattern of air pollution. Zhu et al. [89] integrated the Granger causality test into Bayesian learning to identify the causal networks of air pollution sensors. However, none of the existing methods can accurately detect the dynamic causality of the urban time series, let alone interpret and validate the causality."
  },
  {
    "sentence": "Basic charts like line and bar charts are not suitable for causal graphs.",
    "subtitle": "2.2 Visual Causal Analytics",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 114,
    "para_id": 17,
    "paper_id": 0,
    "2d_coord": [
      0.3696150481700897,
      -1.667496919631958
    ],
    "MSU_id": 114,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Causal analysis needs effective causality representations. Visualization studies have investigated causality perception. Xiong et al. [78] and Yen et al. [81] evaluated the performance of some basic charts (e.g., line and bar charts) in conveying causality. These basic charts are not suitable for causal graphs. Bae et al. [6, 7] derived several design guidelines for the graphical representation of causality. These guidelines help us design effective causal visualizations."
  },
  {
    "sentence": "The existing visualizations and systems cannot accommodate dynamic causal relations.",
    "subtitle": "2.2 Visual Causal Analytics",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 132,
    "para_id": 20,
    "paper_id": 0,
    "2d_coord": [
      -0.4615987539291382,
      -2.3875317573547363
    ],
    "MSU_id": 132,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "The existing visualizations and systems cannot accommodate dynamic causal relations. Besides, visual causal analysis of time series has not been well studied. We propose a visual analytics approach for analyzing the dynamic causal relations detected from urban time series."
  },
  {
    "sentence": "Visual causal analysis of time series has not been well studied.",
    "subtitle": "2.2 Visual Causal Analytics",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 133,
    "para_id": 20,
    "paper_id": 0,
    "2d_coord": [
      0.3980332016944885,
      -1.2138619422912598
    ],
    "MSU_id": 133,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "The existing visualizations and systems cannot accommodate dynamic causal relations. Besides, visual causal analysis of time series has not been well studied. We propose a visual analytics approach for analyzing the dynamic causal relations detected from urban time series."
  },
  {
    "sentence": "Liu et al. combined trajectory mining and visualizations to evaluate appropriate locations for billboards.",
    "subtitle": "2.3 Visual Urban Analytics",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 140,
    "para_id": 22,
    "paper_id": 0,
    "2d_coord": [
      0.3350852429866791,
      -1.6871044635772705
    ],
    "MSU_id": 140,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Visual urban planning focuses on decision making for improved cities [23, 37, 43, 50], e.g., by optimizing the arrangement of facilities. For example, Liu et al. [40] combined trajectory mining and visualizations to evaluate appropriate locations for billboards. Weng et al. [73] proposed an interactive approach for determining critical locations using computed reachability. Weng et al. [72] developed a visual analytics system for improving bus routes with Monte Carlo search."
  },
  {
    "sentence": "Weng et al. proposed an interactive approach for determining critical locations using computed reachability.",
    "subtitle": "2.3 Visual Urban Analytics",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 141,
    "para_id": 22,
    "paper_id": 0,
    "2d_coord": [
      -0.406157523393631,
      -2.297369956970215
    ],
    "MSU_id": 141,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Visual urban planning focuses on decision making for improved cities [23, 37, 43, 50], e.g., by optimizing the arrangement of facilities. For example, Liu et al. [40] combined trajectory mining and visualizations to evaluate appropriate locations for billboards. Weng et al. [73] proposed an interactive approach for determining critical locations using computed reachability. Weng et al. [72] developed a visual analytics system for improving bus routes with Monte Carlo search."
  },
  {
    "sentence": "Weng et al. developed a visual analytics system for improving bus routes with Monte Carlo search.",
    "subtitle": "2.3 Visual Urban Analytics",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 142,
    "para_id": 22,
    "paper_id": 0,
    "2d_coord": [
      0.38486447930336,
      -1.4262611865997314
    ],
    "MSU_id": 142,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Visual urban planning focuses on decision making for improved cities [23, 37, 43, 50], e.g., by optimizing the arrangement of facilities. For example, Liu et al. [40] combined trajectory mining and visualizations to evaluate appropriate locations for billboards. Weng et al. [73] proposed an interactive approach for determining critical locations using computed reachability. Weng et al. [72] developed a visual analytics system for improving bus routes with Monte Carlo search."
  },
  {
    "sentence": "The detected static results can be rough or even incorrect.",
    "subtitle": "3.1 Problem Formulation",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 167,
    "para_id": 27,
    "paper_id": 0,
    "2d_coord": [
      -3.15600323677063,
      -1.001564383506775
    ],
    "MSU_id": 167,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "After preliminary discussions, we realize that the causal relations in urban space can be dynamic. Take Fig. 2 as an example. The causal direction between sensors  $s_1$  and  $s_2$  disappears and is flipped over time (Fig. 2a). Although causal detection models can summarize these causal relations across a whole time span, the detected static results can be rough (partially correct) or even incorrect and provide limited insights into precise and reliable policymaking (Fig. 2b and 2c). No interactive systems are available for interpreting and validating the detected causality in an urban context, which makes the causality unreliable and cannot be fully utilized (Fig. 2d)."
  },
  {
    "sentence": "Static results provide limited insights into precise and reliable policymaking.",
    "subtitle": "3.1 Problem Formulation",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 168,
    "para_id": 27,
    "paper_id": 0,
    "2d_coord": [
      -2.093723773956299,
      -1.059353232383728
    ],
    "MSU_id": 168,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "After preliminary discussions, we realize that the causal relations in urban space can be dynamic. Take Fig. 2 as an example. The causal direction between sensors  $s_1$  and  $s_2$  disappears and is flipped over time (Fig. 2a). Although causal detection models can summarize these causal relations across a whole time span, the detected static results can be rough (partially correct) or even incorrect and provide limited insights into precise and reliable policymaking (Fig. 2b and 2c). No interactive systems are available for interpreting and validating the detected causality in an urban context, which makes the causality unreliable and cannot be fully utilized (Fig. 2d)."
  },
  {
    "sentence": "Experts commented that detection results are not completely reliable.",
    "subtitle": "3.3 Workflow and Requirement Analysis",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 219,
    "para_id": 38,
    "paper_id": 0,
    "2d_coord": [
      -0.8830235600471497,
      0.39813461899757385
    ],
    "MSU_id": 219,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "R5 Modify incorrect causal relations (WI). The experts commented that detection results are not completely reliable. Causal interpretation and validation can help identify incorrect causal relations. Afterward, the experts require modifying them interactively."
  },
  {
    "sentence": "All edges in the design are connected to the red node, causing visual clutter.",
    "subtitle": "5.3 Graph View",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 414,
    "para_id": 72,
    "paper_id": 0,
    "2d_coord": [
      -0.9972957372665405,
      -2.3649892807006836
    ],
    "MSU_id": 414,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Justification. Many timeline- based and scalable visualizations for dynamic graphs have been proposed [24]. In Beck et al.'s hierarchical taxonomy [6], juxtaposed node- link diagrams [11] are the most suitable for our problem. We have created an alternative design using juxtaposed node- link diagrams (Fig. 7a- 1). However, all edges are connected to the red node (i.e., the ego sensor), causing visual clutter. The variations of the causal directions cannot be easily revealed. We have also created an alternative (Fig. 7a- 2). Each row represents a neighbor sensor or its causal relations with the ego sensor. Each rectangle is divided into two parts to indicate the opposite directions. Although the causal relations are revealed without clutter, the spatial structures are still lost. We enhance this design and derive our final design (Fig. 1c) by using the compass glyphs and adding the heatmaps and the minimaps."
  },
  {
    "sentence": "The variations of the causal directions cannot be easily revealed in the initial design.",
    "subtitle": "5.3 Graph View",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 415,
    "para_id": 72,
    "paper_id": 0,
    "2d_coord": [
      -1.6349971294403076,
      -2.0859973430633545
    ],
    "MSU_id": 415,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Justification. Many timeline- based and scalable visualizations for dynamic graphs have been proposed [24]. In Beck et al.'s hierarchical taxonomy [6], juxtaposed node- link diagrams [11] are the most suitable for our problem. We have created an alternative design using juxtaposed node- link diagrams (Fig. 7a- 1). However, all edges are connected to the red node (i.e., the ego sensor), causing visual clutter. The variations of the causal directions cannot be easily revealed. We have also created an alternative (Fig. 7a- 2). Each row represents a neighbor sensor or its causal relations with the ego sensor. Each rectangle is divided into two parts to indicate the opposite directions. Although the causal relations are revealed without clutter, the spatial structures are still lost. We enhance this design and derive our final design (Fig. 1c) by using the compass glyphs and adding the heatmaps and the minimaps."
  },
  {
    "sentence": "Although the causal relations are revealed without clutter, the spatial structures are still lost.",
    "subtitle": "5.3 Graph View",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 418,
    "para_id": 72,
    "paper_id": 0,
    "2d_coord": [
      -0.8208780884742737,
      -2.4027111530303955
    ],
    "MSU_id": 418,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Justification. Many timeline- based and scalable visualizations for dynamic graphs have been proposed [24]. In Beck et al.'s hierarchical taxonomy [6], juxtaposed node- link diagrams [11] are the most suitable for our problem. We have created an alternative design using juxtaposed node- link diagrams (Fig. 7a- 1). However, all edges are connected to the red node (i.e., the ego sensor), causing visual clutter. The variations of the causal directions cannot be easily revealed. We have also created an alternative (Fig. 7a- 2). Each row represents a neighbor sensor or its causal relations with the ego sensor. Each rectangle is divided into two parts to indicate the opposite directions. Although the causal relations are revealed without clutter, the spatial structures are still lost. We enhance this design and derive our final design (Fig. 1c) by using the compass glyphs and adding the heatmaps and the minimaps."
  },
  {
    "sentence": "A causal link may be incorrect because the Pearson's $r$ is near 0.",
    "subtitle": "5.4 Relation View",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 424,
    "para_id": 74,
    "paper_id": 0,
    "2d_coord": [
      -0.32523205876350403,
      -2.1273791790008545
    ],
    "MSU_id": 424,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Fig. 6. A causal relation (left) and the relation view for visualizing causal relations (right). (a) A causal link may be incorrect because the Pearson's  $r$  is near 0. (b) A causal link opposite to (a) has a large Pearson's  $r$ . (c) A bi-directional causal relation comprises two causal links with opposite directions. (d) The relation view adopts a multidimensional visualization and presents (d1, d2, and d3) three illustrative causal relations."
  },
  {
    "sentence": "The spreadsheet cannot handle many relations.",
    "subtitle": "5.4 Relation View",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 451,
    "para_id": 77,
    "paper_id": 0,
    "2d_coord": [
      0.14941297471523285,
      -1.4350686073303223
    ],
    "MSU_id": 451,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Justification. Each causal relation is an essentially multidimensional datum with seven dimensions (Fig. 7b- 1). Various visualizations have been proposed for this kind of data. Such as dimension reduction [68,88], scatterplot matrix [49], parallel coordinate plot (PCP) [10] and glyphs [82]. This view should satisfy two requirements, showing details and revealing suspiciousness. We have created two alternatives after exploring the design space. The first one is the spreadsheet that is commonly seen as a detail view (Fig. 7b- 1). However, it cannot handle many relations. The second one is the PCP (Fig. 7b- 2) better than the spreadsheet. Numerous causal relations can be displayed. The values of each dimension can be seen in the parallel axes. The first type of suspicious relations can be indicated by the gray lines passing the top of the distance axis (Fig. 7b- 2). Built on the PCP, we adopt scatterplots to visualize the dimensions related to the same causal direction (Fig. 6d). Scatterplots allow another suspiciousness metric, the Pearson's  $r$ , to be encoded using the size that can be accurately perceived. Besides, scatterplots are more familiar to the experts than the PCP."
  },
  {
    "sentence": "Some involved causal links were suspicious because they had low correlation coefficients.",
    "subtitle": "6.1 Causal Analysis of Air Pollution Time Series",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 528,
    "para_id": 91,
    "paper_id": 0,
    "2d_coord": [
      0.09776298701763153,
      -1.975907802581787
    ],
    "MSU_id": 528,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "A sharp peak in the third time window was first noticed. The heatmap below showed that the stimulus of an air pollution event spread from north to south (Fig. 1e- 1). After hovering over this band, the map view displayed the spatial causal graph (Fig. 1a- 1). However, some involved causal links were suspicious because they had low correlation coefficients indicated by the circle sizes in the relation view (Fig. 1d- 2). Given that, the experts unfolded this graph band to verify the causalities (Fig. 1c- 2). The orders in which the peaks appear in the two time series supported the causal directions detected by the model. The time differences of the peaks were also consistent with the lags. EB said, \"although the time series were not highly correlated, the model captured the key features, i.e., the peaks, and obtained reasonable causalities.\""
  },
  {
    "sentence": "A static causal graph cannot support precise source identification and better policymaking.",
    "subtitle": "6.1 Causal Analysis of Air Pollution Time Series",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 568,
    "para_id": 98,
    "paper_id": 0,
    "2d_coord": [
      -0.6827207803726196,
      -2.1439461708068848
    ],
    "MSU_id": 568,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "We showcased that the two most serious air pollution in Beijing were caused by the northwest and southeast areas, respectively. The time- oriented analysis disclosed these fine- grained and dynamic causalities that shed light on precise source identification and better policymaking that a static causal graph cannot support."
  },
  {
    "sentence": "The static causal graphs detected by the previous methods will ignore these time-varying cause-effect relationships.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 628,
    "para_id": 109,
    "paper_id": 0,
    "2d_coord": [
      -0.21767446398735046,
      -1.816863775253296
    ],
    "MSU_id": 628,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Implications. This study is the first to extend the visual causal analysis to urban scenarios. First, we are aware of the dynamics of causality in urban environments. A causal detection framework and temporal visualizations are combined to unveil these dynamic causalities. The case studies demonstrate the dynamic causalities in air pollution phenomena and traffic situations. The static causal graphs detected by the previous methods will ignore these time- varying cause- effect relationships and thus cannot support accurate and informed decisions. Second, we observe the causalities detected by automatic models are not always reliable in urban domains. This observation supports other researchers' opinion [31, 76] that human knowledge needs to be integrated into the analysis loop for reliable analyses."
  },
  {
    "sentence": "Static causal graphs cannot support accurate and informed decisions.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 629,
    "para_id": 109,
    "paper_id": 0,
    "2d_coord": [
      -0.6919345855712891,
      -2.378283977508545
    ],
    "MSU_id": 629,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Implications. This study is the first to extend the visual causal analysis to urban scenarios. First, we are aware of the dynamics of causality in urban environments. A causal detection framework and temporal visualizations are combined to unveil these dynamic causalities. The case studies demonstrate the dynamic causalities in air pollution phenomena and traffic situations. The static causal graphs detected by the previous methods will ignore these time- varying cause- effect relationships and thus cannot support accurate and informed decisions. Second, we observe the causalities detected by automatic models are not always reliable in urban domains. This observation supports other researchers' opinion [31, 76] that human knowledge needs to be integrated into the analysis loop for reliable analyses."
  },
  {
    "sentence": "Some recognized issues of the Granger causality test still exist, such as it being a linear model.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 638,
    "para_id": 110,
    "paper_id": 0,
    "2d_coord": [
      -2.8629953861236572,
      -1.838592767715454
    ],
    "MSU_id": 638,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Generalizability. The generalizability of Compass is two- fold. In terms of application, the two real- world case studies have demonstrated that Compass can be easily applied to the urban time series data from different domains. Other potential applications include analyzing noise pollution [9], water pollution [1], and complex physical systems (e.g., IoT systems of tree ecosystem services [46]). For example, a river can be polluted by its upstream tributaries. Compass can help identify when the river is polluted and reveal which tributaries pollute it for each period when the pollution events are observed. In terms of visual analytics, Compass can be regarded as a model- free visual analytics approach and is not limited to the Granger causality test. Granger causality test is currently the most suitable method for our problem because of its wide use, interpretability, and ability to detect causality based on short partitioned time series. However, some recognized issues of the Granger causality test still exist. For example, it is a linear model [27]. Any better model proposed in the future can be easily integrated into our visual analytics system."
  },
  {
    "sentence": "Three limitations are observed in this study.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 640,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -0.6761960387229919,
      -2.303901433944702
    ],
    "MSU_id": 640,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "The system cannot support the analysis of complex causal graphs.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 641,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -0.3852466642856598,
      -2.4449386596679688
    ],
    "MSU_id": 641,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "Causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 643,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -0.745917797088623,
      -1.687697172164917
    ],
    "MSU_id": 643,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "Visual clutter is hard to avoid because the geo-fixed nodes make the causal graph layout methods inapplicable.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 645,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -0.8942724466323853,
      -2.1575989723205566
    ],
    "MSU_id": 645,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "It becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 646,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -2.2509546279907227,
      -1.5774203538894653
    ],
    "MSU_id": 646,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "Due to the lack of multi-source datasets in the same temporal and spatial scope, the effectiveness of our method in cross-domain causal analysis has not been proven.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 648,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -1.7915750741958618,
      -0.4481675326824188
    ],
    "MSU_id": 648,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "When the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds.",
    "subtitle": "7 DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 652,
    "para_id": 111,
    "paper_id": 0,
    "2d_coord": [
      -2.0377888679504395,
      -1.368841290473938
    ],
    "MSU_id": 652,
    "paper_info": "Compass Towards Better Causal Analysis of Urban Time Series",
    "paragraph_info": "Limitations and future work. Three limitations are observed in this study. First, the system cannot support the analysis of complex causal graphs. Although ego graph- like causal graphs support an effective analysis for the target area, causal analyses can be enhanced with more complex causal graphs, such as by involving the relations between neighbor sensors. However, analyzing the complex graphs in a spatiotemporal context is challenging. For example, visual clutter is hard to avoid because the geo- fixed nodes make the causal graph layout methods inapplicable [65,76]. It also becomes difficult to clearly reveal the temporal variations of the detected causalities as the number of edges increases. Utilizing the third dimension can be a potential solution [5, 14, 62, 80] and deserves further study. Secondly, due to the lack of multi- source datasets in the same temporal and spatial scope, the effectiveness of our method in cross- domain causal analysis has not been proven. In fact, causality may exist between different urban datasets. For example, as the traffic volume increases, the air quality will deteriorate because cars emit exhaust gas and pollute the air [83]. In the future, we will collect richer datasets and conduct studies, thereby evaluating and improving our approaches. Finally, we found that when the number of links to be tested in all time windows is more than 150, the calculation time exceeds ten seconds. In the future, we will optimize the detection model, implement it with a more high- performance programming language, and deploy it to a distributed computing server."
  },
  {
    "sentence": "Considerable uncertainties and model assumptions related to emissions, meteorological conditions, and chemical reaction mechanisms lead to imperfections in these models.",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 691,
    "para_id": 116,
    "paper_id": 1,
    "2d_coord": [
      -2.097862482070923,
      -0.13013100624084473
    ],
    "MSU_id": 691,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "In order to better manage and prevent air pollution processes, it is important to improve the prediction accuracy for pollutants such as  $\\mathrm{PM}_{2.5}$ , based on numerical models (Chen et al., 2017; Ma et al., 2020). Numerical models mainly realize their forecasts by combining the consideration of pollutant emissions, meteorological conditions, and physicochemical reactions. With the continuous research and development of numerical models, many regional and global atmospheric chemistry models, such as the Weather Research and Forecasting model coupled with Chemistry (WRF- Chem; Grell et al., 2005), the Community Multiscale Air Quality model (CMAQ, https://www.epa.gov/cmaq), the Nested Air Quality Prediction Modeling System (NAQPMS; Wang et al., 2001), and the Global Modeling and Assimilation Office (GEOS- Chem, https://www.geos- chem.org), have been developed in recent years. However, due to the considerable uncertainties and model assumptions related to pollutant emissions, meteorological conditions, and chemical reaction mechanisms, the frameworks and parameterization schemes of these models are still imperfect, which may bring errors in their forecasts (Van Loon et al., 2007; Zhu et al., 2018; Lv et al., 2018)."
  },
  {
    "sentence": "Imperfect frameworks and parameterization schemes in atmospheric chemistry models may cause errors in forecasts.",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 692,
    "para_id": 116,
    "paper_id": 1,
    "2d_coord": [
      -2.852572441101074,
      0.21292415261268616
    ],
    "MSU_id": 692,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "In order to better manage and prevent air pollution processes, it is important to improve the prediction accuracy for pollutants such as  $\\mathrm{PM}_{2.5}$ , based on numerical models (Chen et al., 2017; Ma et al., 2020). Numerical models mainly realize their forecasts by combining the consideration of pollutant emissions, meteorological conditions, and physicochemical reactions. With the continuous research and development of numerical models, many regional and global atmospheric chemistry models, such as the Weather Research and Forecasting model coupled with Chemistry (WRF- Chem; Grell et al., 2005), the Community Multiscale Air Quality model (CMAQ, https://www.epa.gov/cmaq), the Nested Air Quality Prediction Modeling System (NAQPMS; Wang et al., 2001), and the Global Modeling and Assimilation Office (GEOS- Chem, https://www.geos- chem.org), have been developed in recent years. However, due to the considerable uncertainties and model assumptions related to pollutant emissions, meteorological conditions, and chemical reaction mechanisms, the frameworks and parameterization schemes of these models are still imperfect, which may bring errors in their forecasts (Van Loon et al., 2007; Zhu et al., 2018; Lv et al., 2018)."
  },
  {
    "sentence": "The improvement of the prediction is limited, and the effect of assimilation gradually disappears with long-term integration of the simulation (Ma et al., 2024; Ma et al., 2020; Hong et al., 2022).",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 720,
    "para_id": 121,
    "paper_id": 1,
    "2d_coord": [
      -3.171082019805908,
      -0.2083200365304947
    ],
    "MSU_id": 720,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Schwartz et al. (2012) used three different assimilation methods with MODIS (Moderate Resolution Imaging Spectroradiometer) AOD (aerosol optical depth) products and in situ  $\\mathrm{PM}{2.5}$  observations, and the results showed that all three methods improved the forecasting of aerosols. Song et al. (2021) used the 3DVAR (three- dimensional variational) approach to jointly assimilate meteorological data and in situ observations to improve the prediction of  $\\mathrm{PM}{2.5}$ . Hong et al. (2022) assimilated in situ observations and satellite data simultaneously to further improve  $\\mathrm{PM}{2.5}$  predictions. And in another study, different kinds of data were used in the assimilation of the initial field to analyze the enhancement of  $\\mathrm{PM}{2.5}$  forecasts by different assimilation sources (Ma et al., 2024). These previous studies have shown that DA can improve  $\\mathrm{PM}_{2.5}$  predictions by adjusting the initial state of the atmospheric chemistry, but the improvement of the prediction is limited, and the effect of assimilation gradually disappears with long- term integration of the simulation (Ma et al., 2024; Ma et al., 2020; Hong et al., 2022). In addition, the model prediction errors are not only affected by the uncertainty of the initial field, but also various other uncertainties such as those of the meteorological field and the physicochemical reactions (Zhu et al., 2018). Numerical model BC comprehensively considers the systematic errors of the model forecast to improve the forecast accuracy. Different from DA, which is the adjustment of the initial field from the model input perspective, BC is the post- processing of model outputs from the back end of the model, and can to a certain extent make up for the limitations of initial field DA."
  },
  {
    "sentence": "The model prediction errors are not only affected by the uncertainty of the initial field, but also various other uncertainties such as those of the meteorological field and the physicochemical reactions (Zhu et al., 2018).",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 721,
    "para_id": 121,
    "paper_id": 1,
    "2d_coord": [
      -3.080047607421875,
      -0.3837296664714813
    ],
    "MSU_id": 721,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Schwartz et al. (2012) used three different assimilation methods with MODIS (Moderate Resolution Imaging Spectroradiometer) AOD (aerosol optical depth) products and in situ  $\\mathrm{PM}{2.5}$  observations, and the results showed that all three methods improved the forecasting of aerosols. Song et al. (2021) used the 3DVAR (three- dimensional variational) approach to jointly assimilate meteorological data and in situ observations to improve the prediction of  $\\mathrm{PM}{2.5}$ . Hong et al. (2022) assimilated in situ observations and satellite data simultaneously to further improve  $\\mathrm{PM}{2.5}$  predictions. And in another study, different kinds of data were used in the assimilation of the initial field to analyze the enhancement of  $\\mathrm{PM}{2.5}$  forecasts by different assimilation sources (Ma et al., 2024). These previous studies have shown that DA can improve  $\\mathrm{PM}_{2.5}$  predictions by adjusting the initial state of the atmospheric chemistry, but the improvement of the prediction is limited, and the effect of assimilation gradually disappears with long- term integration of the simulation (Ma et al., 2024; Ma et al., 2020; Hong et al., 2022). In addition, the model prediction errors are not only affected by the uncertainty of the initial field, but also various other uncertainties such as those of the meteorological field and the physicochemical reactions (Zhu et al., 2018). Numerical model BC comprehensively considers the systematic errors of the model forecast to improve the forecast accuracy. Different from DA, which is the adjustment of the initial field from the model input perspective, BC is the post- processing of model outputs from the back end of the model, and can to a certain extent make up for the limitations of initial field DA."
  },
  {
    "sentence": "Machine-learning methods usually require massive, multi-source data inputs, which can limit their applicability in specific scenarios.",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 741,
    "para_id": 122,
    "paper_id": 1,
    "2d_coord": [
      -2.519928455352783,
      -0.9640024900436401
    ],
    "MSU_id": 741,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "BC is a statistical method that improves model predictions by establishing a relationship between historical observations and the corresponding model forecasts, from which the forecast bias can be extracted and the systematic errors corrected. With the advancements made in this line of research, BC methods have been developed from the earlier complete forecast method (Klein et al., 1959) to the model output statistical method (Glahn and Lowry, 1972), and then gradually evolved into different BC schemes. Frequency matching, Bayesian averaging, least- squares regression, multiple linear regression, and other statistical methods have been used to establish BC models, which significantly reduce systematic errors by learning the empirical relationships between historical forecasts and observations to realize BC (Raftery et al., 2005; Zhu and Luo, 2015; Xie et al., 2012; Chen et al., 2012; You, 2014). Recently, artificial- intelligence machine- learning techniques have emerged. Subsequently, BC based on machine- learning methods has been shown to better express the auxiliary links between various facets of the atmosphere, including, for instance, the different elements involved in atmospheric chemistry and key basic meteorological variables such as temperature and winds (Marzban, 2003; Zhang et al., 2022). Machine- learning models have been used in the BC of model outputs to improve  $\\mathrm{PM}{2.5}$  forecasts. For example, Ran et al. (2023) performed BC based on a multiple linear regression modeling approach to reduce the global  $\\mathrm{PM}{2.5}$  prediction bias; and Liu and Xing (2022a), Liu and Xing (2022b) designed a fully connected deep neural network to correct the  $\\mathrm{PM}{2.5}$  concentration bias in the outputs of chemical models, achieving better results than without the correction method applied. Ma et al. (2020) applied a time series model to correct  $\\mathrm{PM}{2.5}$  forecasts, which achieved good results. Different machine- learning methods were utilized by Li et al. (2021) as bias adjustment for postprocessing forecasts of  $\\mathrm{PM}{2.5}$  and  $\\mathrm{O}_3$ , showing the optimal approach to be the Random Forest model. Lu et al. (2020) utilized machine- learning methods to improve the accuracy of  $\\mathrm{PM}{2.5}$  forecasts through WRF- Chem in the Chengdu- Chongqing region. Overall, machine- learning methods have shown strong advantages in mining nonlinear relationships, enabling them to perform well in the BC of numerical predictions. However, they usually require massive, multi- source data inputs, which can limit their applicability in specific scenarios."
  },
  {
    "sentence": "Previous studies have tended to analyze the enhancement of model forecasts by DA and BC independently.",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 745,
    "para_id": 123,
    "paper_id": 1,
    "2d_coord": [
      -2.9607720375061035,
      -0.04026287794113159
    ],
    "MSU_id": 745,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "It can be seen that DA on the initial conditions can optimize the forecast accuracy from the model input perspective through reducing the uncertainty of the initial field, while BC can make improvements to predictions by correcting and reducing the forecast errors from the postprocessing point of view. Both methods can improve the accuracy level of pollutant forecasting to a certain extent. Although both methods can improve the forecast accuracy of the model from the perspective of data statistics, previous studies have tended to analyze the enhancement of model forecasts by the two methods independently, with few studies having compared the enhancement effects of these two methods together. In addition, the effect of combining DA on the initial conditions and BC on the optimization of the model forecast has not been studied."
  },
  {
    "sentence": "Few studies have compared the enhancement effects of DA and BC together.",
    "subtitle": "1. Introduction",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 746,
    "para_id": 123,
    "paper_id": 1,
    "2d_coord": [
      -0.8834728598594666,
      0.3410675525665283
    ],
    "MSU_id": 746,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "It can be seen that DA on the initial conditions can optimize the forecast accuracy from the model input perspective through reducing the uncertainty of the initial field, while BC can make improvements to predictions by correcting and reducing the forecast errors from the postprocessing point of view. Both methods can improve the accuracy level of pollutant forecasting to a certain extent. Although both methods can improve the forecast accuracy of the model from the perspective of data statistics, previous studies have tended to analyze the enhancement of model forecasts by the two methods independently, with few studies having compared the enhancement effects of these two methods together. In addition, the effect of combining DA on the initial conditions and BC on the optimization of the model forecast has not been studied."
  },
  {
    "sentence": "GSI 3DVAR shows advantages in its ability to deal with observations that have complex nonlinear relationships with atmospheric state model variables.",
    "subtitle": "2.3.GSI 3DVAR system",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 786,
    "para_id": 131,
    "paper_id": 1,
    "2d_coord": [
      -0.8989658355712891,
      -0.1351458579301834
    ],
    "MSU_id": 786,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Grid Statistical Interpolation (GsI) is a data analysis system that integrates global and regional variational assimilation techniques developed by NCEP (National Centers for Environmental Prediction), and it has been used for DA in operational forecasting in recent decades (Chen et al., 2023; Feng, 2018). GSI 3DVAR is almost the same as traditional 3D variational methods, which are based on the idea of variational DA. However, it shows advantages in its ability to deal with observations that have complex nonlinear relationships with atmospheric state model variables, thus making it possible to assimilate observations with nonlinear relations to atmospheric state quantities. In order to improve aerosol predictions, an aerosol assimilation module was integrated systematically into the GSI 3DVAR system (Pagowski et al., 2014). In this paper, the DA of the initial field is based on the GSI system at 00:00 UTC each day. The GSI system is fitted to the observation and model background field by minimizing the objective function, and the analytical field is the optimal solution for the minimal value of the objective function in equation (1)."
  },
  {
    "sentence": "GSI 3DVAR makes it possible to assimilate observations with nonlinear relations to atmospheric state quantities.",
    "subtitle": "2.3.GSI 3DVAR system",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 787,
    "para_id": 131,
    "paper_id": 1,
    "2d_coord": [
      -2.4821996688842773,
      -0.13617825508117676
    ],
    "MSU_id": 787,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Grid Statistical Interpolation (GsI) is a data analysis system that integrates global and regional variational assimilation techniques developed by NCEP (National Centers for Environmental Prediction), and it has been used for DA in operational forecasting in recent decades (Chen et al., 2023; Feng, 2018). GSI 3DVAR is almost the same as traditional 3D variational methods, which are based on the idea of variational DA. However, it shows advantages in its ability to deal with observations that have complex nonlinear relationships with atmospheric state model variables, thus making it possible to assimilate observations with nonlinear relations to atmospheric state quantities. In order to improve aerosol predictions, an aerosol assimilation module was integrated systematically into the GSI 3DVAR system (Pagowski et al., 2014). In this paper, the DA of the initial field is based on the GSI system at 00:00 UTC each day. The GSI system is fitted to the observation and model background field by minimizing the objective function, and the analytical field is the optimal solution for the minimal value of the objective function in equation (1)."
  },
  {
    "sentence": "The single convolution kernel in the traditional model can only catch some of the features of the pollutants with a fixed scale.",
    "subtitle": "2.4. Deep-learning-based BC",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 812,
    "para_id": 135,
    "paper_id": 1,
    "2d_coord": [
      -3.154695987701416,
      -1.1691313982009888
    ],
    "MSU_id": 812,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "By adding a multiscale convolution module and depth- separable module based on the classical UNet deep- learning segmentation network (Ronneberger et al., 2015), a new deep- learning method was constructed for BC of WRF- Chem predictions, named MDS- UNet (Ma et al.,2024). Actually, a multiscale convolution module was joined to connect and extract different scales of feature maps, and the depth- separable convolution can compress the feature mapping. MDS- UNet shows advantages in multiscale feature learning due to its multiscale convolution module, as compared with the traditional UNet. The single convolution kernel in the traditional model can only catch some of the features of the pollutants with a fixed scale. However, pollution characteristics vary on different spatial scales and multiscale convolution adopts a hierarchical structure with different spatial sizes of convolution kernels to extract the multiscale features by learning feature maps from the image. By merging feature maps of different scale, not only the pollutant concentration values, but also the spatial information on pollutant distributions, can then be obtained. The model has three advantages: acquisition of multiscale information, acquiring surrounding spatial information, and reducing information loss."
  },
  {
    "sentence": "A reason for the overestimation might be that the emissions inventory overestimated the winter emissions in the above regions and the model simulation was relatively more uncertain due to the complex terrain.",
    "subtitle": "3.1. Comparison of forecast errors by different optimization methods",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 866,
    "para_id": 146,
    "paper_id": 1,
    "2d_coord": [
      -1.804034948348999,
      -0.10396061837673187
    ],
    "MSU_id": 866,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Fig. 4 presents the mean bias distributions of  $\\mathrm{PM}{2.5}$  predictions produced by the different experiments. As can be seen, WRF- Chem overestimated the  $\\mathrm{PM}{2.5}$  concentrations over some areas of SCB, CC, and YRD, with the prediction MB values reaching  $60\\mu \\mathrm{g} / \\mathrm{m}^3$  in some areas. A reason for this might be that the emissions inventory overestimated the winter emissions in the above regions and the model simulation was relatively more uncertain due to the complex terrain (Wei et al., 2022). In contrast, the  $\\mathrm{PM}{2.5}$  concentrations were underestimated in BTH, with the prediction MB ranging from  $- 60$  to  $- 20\\mu \\mathrm{g} / \\mathrm{m}^3$ . This performance whereby WRF- Chem always underestimates  $\\mathrm{PM}{2.5}$  concentrations over northern areas in China is a common problem, caused by not simulating secondary aerosols or the transformation mechanisms with their gaseous precursors (Ma, 2020; Wei et al., 2022; Qin et al., 2020; Wang et al., 2016). The prediction MB values were reduced by the DA and BC methods, but BC had better effects in terms of prediction improvement. Consistent with the results shown in Fig. 3, despite the adjustment to the original overestimation by DA in WRF- Chem_DA over some regions of SCB and CC, the prediction MB values could still reach  $60\\mu \\mathrm{g} / \\mathrm{m}^3$  in some areas, thus demonstrating some advantages of DA, but with limitations. Compared with the direct predictions of WRF- Chem, the prediction MB values in the WRF- Chem_BC experiment were reduced over most areas due to BC effects. It is worth noting that the original overestimation changed to an underestimation in the WRF- Chem_BC experiment over some regions of YRD and the Shandong Peninsula, which indicated overcorrection by BC. For WRF- Chem_DA_BC, the prediction MB was the smallest among all the experiments, with values less than  $20\\mu \\mathrm{g} / \\mathrm{m}^3$  over most areas. WRF- Chem_DA_BC showed a smaller prediction MB than WRF- Chem_BC, especially over the Shandong Peninsula and YRD regions."
  },
  {
    "sentence": "WRF-Chem always underestimates PM2.5 concentrations over northern areas in China due to not simulating secondary aerosols or the transformation mechanisms with their gaseous precursors.",
    "subtitle": "3.1. Comparison of forecast errors by different optimization methods",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 868,
    "para_id": 146,
    "paper_id": 1,
    "2d_coord": [
      -2.9265921115875244,
      0.41677072644233704
    ],
    "MSU_id": 868,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Fig. 4 presents the mean bias distributions of  $\\mathrm{PM}{2.5}$  predictions produced by the different experiments. As can be seen, WRF- Chem overestimated the  $\\mathrm{PM}{2.5}$  concentrations over some areas of SCB, CC, and YRD, with the prediction MB values reaching  $60\\mu \\mathrm{g} / \\mathrm{m}^3$  in some areas. A reason for this might be that the emissions inventory overestimated the winter emissions in the above regions and the model simulation was relatively more uncertain due to the complex terrain (Wei et al., 2022). In contrast, the  $\\mathrm{PM}{2.5}$  concentrations were underestimated in BTH, with the prediction MB ranging from  $- 60$  to  $- 20\\mu \\mathrm{g} / \\mathrm{m}^3$ . This performance whereby WRF- Chem always underestimates  $\\mathrm{PM}{2.5}$  concentrations over northern areas in China is a common problem, caused by not simulating secondary aerosols or the transformation mechanisms with their gaseous precursors (Ma, 2020; Wei et al., 2022; Qin et al., 2020; Wang et al., 2016). The prediction MB values were reduced by the DA and BC methods, but BC had better effects in terms of prediction improvement. Consistent with the results shown in Fig. 3, despite the adjustment to the original overestimation by DA in WRF- Chem_DA over some regions of SCB and CC, the prediction MB values could still reach  $60\\mu \\mathrm{g} / \\mathrm{m}^3$  in some areas, thus demonstrating some advantages of DA, but with limitations. Compared with the direct predictions of WRF- Chem, the prediction MB values in the WRF- Chem_BC experiment were reduced over most areas due to BC effects. It is worth noting that the original overestimation changed to an underestimation in the WRF- Chem_BC experiment over some regions of YRD and the Shandong Peninsula, which indicated overcorrection by BC. For WRF- Chem_DA_BC, the prediction MB was the smallest among all the experiments, with values less than  $20\\mu \\mathrm{g} / \\mathrm{m}^3$  over most areas. WRF- Chem_DA_BC showed a smaller prediction MB than WRF- Chem_BC, especially over the Shandong Peninsula and YRD regions."
  },
  {
    "sentence": "The control experiment demonstrates poor ability in simulating the high-pollution zones.",
    "subtitle": "3.2. Comparison of the forecast consistency with observations achieved by different optimization methods",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 894,
    "para_id": 151,
    "paper_id": 1,
    "2d_coord": [
      -3.0403051376342773,
      -1.3029829263687134
    ],
    "MSU_id": 894,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "The average  $\\mathrm{PM}{2.5}$  concentrations predicted by the four experiments and observed in the study period are given in Fig. 6. From the scatter fitting analysis results, WRF- Chem_DA_BC performed best in its consistency with observations following optimization. The results predicted directly by WRF- Chem in the control experiment were dispersed, with some isolated overestimation and underestimation scatter points fitted along the slope of 0.56, which demonstrates poor ability in simulating the high- pollution zones. By applying DA, the predictions of WRF- Chem_DA were partially improved, with an increased fitting slope of 0.69, showing less overestimation and underestimation through the adjustment of the initial field. The optimization of WRF- Chem_BC was much more evident than that of WRF- Chem_DA compared with the control experiment since the overestimation of the model was adjusted and the overall scatter distributions were more concentrated. However, there still existed an overall underestimation trend, with a slope of 0.73 after BC, which may not be sufficient for the simulation of strong pollution processes. It can be seen from Fig. 6a and 6d that the results of WRF- Chem_DA_BC were greatly improved by combining the two optimization methods, with the fitted slope of 1.04 being close to the straight line of  $k = 1$ . In this experiment, the  $\\mathrm{PM}{2.5}$  predictions were well adjusted and almost all of the high pollution could be captured, showing the best agreement with the observations. Overall, through adjustment of the model input field and correction of the model output, the predictions were effectively improved. It should be noted that the slope values in the control experiment and WRF- Chem_DA are close to those of previous results (Feng et al., 2018; Ma et al., 2024)."
  },
  {
    "sentence": "There still existed an overall underestimation trend, with a slope of 0.73 after BC, which may not be sufficient for the simulation of strong pollution processes.",
    "subtitle": "3.2. Comparison of the forecast consistency with observations achieved by different optimization methods",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 899,
    "para_id": 151,
    "paper_id": 1,
    "2d_coord": [
      -0.9198160767555237,
      -1.1114253997802734
    ],
    "MSU_id": 899,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "The average  $\\mathrm{PM}{2.5}$  concentrations predicted by the four experiments and observed in the study period are given in Fig. 6. From the scatter fitting analysis results, WRF- Chem_DA_BC performed best in its consistency with observations following optimization. The results predicted directly by WRF- Chem in the control experiment were dispersed, with some isolated overestimation and underestimation scatter points fitted along the slope of 0.56, which demonstrates poor ability in simulating the high- pollution zones. By applying DA, the predictions of WRF- Chem_DA were partially improved, with an increased fitting slope of 0.69, showing less overestimation and underestimation through the adjustment of the initial field. The optimization of WRF- Chem_BC was much more evident than that of WRF- Chem_DA compared with the control experiment since the overestimation of the model was adjusted and the overall scatter distributions were more concentrated. However, there still existed an overall underestimation trend, with a slope of 0.73 after BC, which may not be sufficient for the simulation of strong pollution processes. It can be seen from Fig. 6a and 6d that the results of WRF- Chem_DA_BC were greatly improved by combining the two optimization methods, with the fitted slope of 1.04 being close to the straight line of  $k = 1$ . In this experiment, the  $\\mathrm{PM}{2.5}$  predictions were well adjusted and almost all of the high pollution could be captured, showing the best agreement with the observations. Overall, through adjustment of the model input field and correction of the model output, the predictions were effectively improved. It should be noted that the slope values in the control experiment and WRF- Chem_DA are close to those of previous results (Feng et al., 2018; Ma et al., 2024)."
  },
  {
    "sentence": "The effects of DA may decrease with the increase in the forecasting period.",
    "subtitle": "3.3. Influences of the different experimental configurations",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 945,
    "para_id": 157,
    "paper_id": 1,
    "2d_coord": [
      -2.628048896789551,
      0.037014439702034
    ],
    "MSU_id": 945,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "From Fig. 3 and Fig. 4, we can see that the prediction RMSEs of the control experiment were relatively larger in BTH due to its underestimation. The improvement of  $\\mathrm{PM}{2.5}$  predictions in BTH in terms of RMSE reductions by BC were more obvious than those by DA, with regional- average normalized RMSEs of 0.78- 0.87 and 0.41- 0.72, respectively, which was consistent with the RMSE and MB distributions. From the perspective of the regional- average RMSE, WRF- Chem_DA_BC produced a reduction in values of  $24.74 - 78.25\\mu \\mathrm{g} / \\mathrm{m}^3$  due to the combination of DA and BC effects, which shows its advantages over the other experiments in the BTH region. Both DA and BC were able to make adjustments to the overestimation of the  $\\mathrm{PM}{2.5}$  predictions in the control experiment in YRD and CC. However, despite their similar effects in the first  $10\\mathrm{h}$ , the optimization of BC was much better than DA in the later forecasting period since the effects of DA may decrease with the increase in the forecasting period. In SCB, the optimization of DA was not as good as that in CC and YRD due to the larger uncertainty caused by the complex terrain here (Wei et al., 2022), which caused the relatively similar effects of the WRF- Chem_BC and WRF- Chem_DA_BC experiments here. Overall, the effects of DA over YRD and CC were more obvious than over BTH and SCB. BC led to good performances over the whole 0- 24- h forecasting period, while DA effects may diminish with the increase in forecasting hours. By combining DA and BC in the WRF- Chem_DA_BC experiment, the consistency between  $\\mathrm{PM}{2.5}$  predictions and observations was improved greatly, showing the best forecast effects. The normalized RMSEs produced by WRF- Chem_DA_BC over BTH, YRD, CC, and SCB were 0.38- 0.68, 0.43- 0.61, 0.60- 0.72, and 0.51- 0.70, respectively. These prediction statistics in different regions also indicate the effectiveness and applicability of the combination of BC and DA in improving  $\\mathrm{PM}{2.5}$  forecasts."
  },
  {
    "sentence": "In SCB, the optimization of DA was not as good as that in CC and YRD due to the larger uncertainty caused by the complex terrain.",
    "subtitle": "3.3. Influences of the different experimental configurations",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 946,
    "para_id": 157,
    "paper_id": 1,
    "2d_coord": [
      -3.1840996742248535,
      -0.19825202226638794
    ],
    "MSU_id": 946,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "From Fig. 3 and Fig. 4, we can see that the prediction RMSEs of the control experiment were relatively larger in BTH due to its underestimation. The improvement of  $\\mathrm{PM}{2.5}$  predictions in BTH in terms of RMSE reductions by BC were more obvious than those by DA, with regional- average normalized RMSEs of 0.78- 0.87 and 0.41- 0.72, respectively, which was consistent with the RMSE and MB distributions. From the perspective of the regional- average RMSE, WRF- Chem_DA_BC produced a reduction in values of  $24.74 - 78.25\\mu \\mathrm{g} / \\mathrm{m}^3$  due to the combination of DA and BC effects, which shows its advantages over the other experiments in the BTH region. Both DA and BC were able to make adjustments to the overestimation of the  $\\mathrm{PM}{2.5}$  predictions in the control experiment in YRD and CC. However, despite their similar effects in the first  $10\\mathrm{h}$ , the optimization of BC was much better than DA in the later forecasting period since the effects of DA may decrease with the increase in the forecasting period. In SCB, the optimization of DA was not as good as that in CC and YRD due to the larger uncertainty caused by the complex terrain here (Wei et al., 2022), which caused the relatively similar effects of the WRF- Chem_BC and WRF- Chem_DA_BC experiments here. Overall, the effects of DA over YRD and CC were more obvious than over BTH and SCB. BC led to good performances over the whole 0- 24- h forecasting period, while DA effects may diminish with the increase in forecasting hours. By combining DA and BC in the WRF- Chem_DA_BC experiment, the consistency between  $\\mathrm{PM}{2.5}$  predictions and observations was improved greatly, showing the best forecast effects. The normalized RMSEs produced by WRF- Chem_DA_BC over BTH, YRD, CC, and SCB were 0.38- 0.68, 0.43- 0.61, 0.60- 0.72, and 0.51- 0.70, respectively. These prediction statistics in different regions also indicate the effectiveness and applicability of the combination of BC and DA in improving  $\\mathrm{PM}{2.5}$  forecasts."
  },
  {
    "sentence": "DA effects may diminish with the increase in forecasting hours.",
    "subtitle": "3.3. Influences of the different experimental configurations",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 950,
    "para_id": 157,
    "paper_id": 1,
    "2d_coord": [
      -3.121586799621582,
      -0.20794472098350525
    ],
    "MSU_id": 950,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "From Fig. 3 and Fig. 4, we can see that the prediction RMSEs of the control experiment were relatively larger in BTH due to its underestimation. The improvement of  $\\mathrm{PM}{2.5}$  predictions in BTH in terms of RMSE reductions by BC were more obvious than those by DA, with regional- average normalized RMSEs of 0.78- 0.87 and 0.41- 0.72, respectively, which was consistent with the RMSE and MB distributions. From the perspective of the regional- average RMSE, WRF- Chem_DA_BC produced a reduction in values of  $24.74 - 78.25\\mu \\mathrm{g} / \\mathrm{m}^3$  due to the combination of DA and BC effects, which shows its advantages over the other experiments in the BTH region. Both DA and BC were able to make adjustments to the overestimation of the  $\\mathrm{PM}{2.5}$  predictions in the control experiment in YRD and CC. However, despite their similar effects in the first  $10\\mathrm{h}$ , the optimization of BC was much better than DA in the later forecasting period since the effects of DA may decrease with the increase in the forecasting period. In SCB, the optimization of DA was not as good as that in CC and YRD due to the larger uncertainty caused by the complex terrain here (Wei et al., 2022), which caused the relatively similar effects of the WRF- Chem_BC and WRF- Chem_DA_BC experiments here. Overall, the effects of DA over YRD and CC were more obvious than over BTH and SCB. BC led to good performances over the whole 0- 24- h forecasting period, while DA effects may diminish with the increase in forecasting hours. By combining DA and BC in the WRF- Chem_DA_BC experiment, the consistency between  $\\mathrm{PM}{2.5}$  predictions and observations was improved greatly, showing the best forecast effects. The normalized RMSEs produced by WRF- Chem_DA_BC over BTH, YRD, CC, and SCB were 0.38- 0.68, 0.43- 0.61, 0.60- 0.72, and 0.51- 0.70, respectively. These prediction statistics in different regions also indicate the effectiveness and applicability of the combination of BC and DA in improving  $\\mathrm{PM}{2.5}$  forecasts."
  },
  {
    "sentence": "There was little or no improvement over some cities in Northeast China due to the lack of observations.",
    "subtitle": "3.4. 2.5.  $\\mathrm{PM}_{2.5}$  prediction results of WRF-Chem_DA_BC",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 978,
    "para_id": 161,
    "paper_id": 1,
    "2d_coord": [
      -2.8130102157592773,
      -1.2200405597686768
    ],
    "MSU_id": 978,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "over central and eastern China; however, the predictions of WRF- Chem_DA_BC showed better consistency with observations. In the control experiment,  $\\mathrm{PM}{2.5}$  predictions were largely overestimated over YRD, CC, and around SCB, and underestimated in Shanxi Province, while the WRF- Chem_DA_BC experiment adjusted the concentrations in these regions. By combining the adjustment of the model input and corrections to the model output, the  $\\mathrm{PM}{2.5}$  prediction distribution and values were much closer to the observations, showing large advantages over these important regions examined in this study (Hong et al., 2022; Jiang et al., 2013; Ma et al., 2024). Despite the overall improvement associated with the combined application of DA and BC, there was still little or no improvement over some cities in Northeast China due to the lack of observations, which will need further research."
  },
  {
    "sentence": "WRF-Chem_DA_BC underestimated the PM2.5 concentrations during a pollution process in February 2019.",
    "subtitle": "3.4. 2.5.  $\\mathrm{PM}_{2.5}$  prediction results of WRF-Chem_DA_BC",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 993,
    "para_id": 163,
    "paper_id": 1,
    "2d_coord": [
      -1.6614569425582886,
      0.31593528389930725
    ],
    "MSU_id": 993,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Time series of the regional- averaged  $\\mathrm{PM}{2.5}$  predictions over the entire domain and the four urban agglomerations are shown in Figure S1 and Figure S2, respectively. From the figures, we can see that some undulation existed during the pollution processes due to the high incidence of winter haze. The results indicate that WRF- Chem_BC_DA reduced the RMSE, improved the time series consistency, and better captured particular pollution episodes, which is in agreement with the statistical results presented in section 3.3. Despite the overall good performances of WRF- Chem_DA_BC in the four regions, it still underestimated the  $\\mathrm{PM}{2.5}$  concentrations during a pollution process in February 2019, which may have been caused by the fixed emissions"
  },
  {
    "sentence": "The underestimation may have been caused by the fixed emissions.",
    "subtitle": "3.4. 2.5.  $\\mathrm{PM}_{2.5}$  prediction results of WRF-Chem_DA_BC",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 994,
    "para_id": 163,
    "paper_id": 1,
    "2d_coord": [
      -0.7448039054870605,
      0.15237057209014893
    ],
    "MSU_id": 994,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Time series of the regional- averaged  $\\mathrm{PM}{2.5}$  predictions over the entire domain and the four urban agglomerations are shown in Figure S1 and Figure S2, respectively. From the figures, we can see that some undulation existed during the pollution processes due to the high incidence of winter haze. The results indicate that WRF- Chem_BC_DA reduced the RMSE, improved the time series consistency, and better captured particular pollution episodes, which is in agreement with the statistical results presented in section 3.3. Despite the overall good performances of WRF- Chem_DA_BC in the four regions, it still underestimated the  $\\mathrm{PM}{2.5}$  concentrations during a pollution process in February 2019, which may have been caused by the fixed emissions"
  },
  {
    "sentence": "We did not give concern to the adjustment of emissions in the current DA experiment.",
    "subtitle": "3.4. 2.5.  $\\mathrm{PM}_{2.5}$  prediction results of WRF-Chem_DA_BC",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1004,
    "para_id": 165,
    "paper_id": 1,
    "2d_coord": [
      -2.238248348236084,
      -0.26798051595687866
    ],
    "MSU_id": 1004,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "inventory we used here. Besides, the predictions produced by WRF- Chem_DA_BC revealed better consistency with observations in BTH and YRD. As such, although the predictions were improved in the WRF- Chem_DA_BC experiment, some overestimation errors still existed, with the regional average concentration being 58.84 and  $56.86\\mu \\mathrm{g} / \\mathrm{m}^3$  for the prediction and observations, respectively. In the current DA experiment, we only considered the adjustment of the initial state of the aerosol fields, without giving concern to the adjustment of emissions, which might have limited the improvement in results."
  },
  {
    "sentence": "Not adjusting emissions might have limited the improvement in results.",
    "subtitle": "3.4. 2.5.  $\\mathrm{PM}_{2.5}$  prediction results of WRF-Chem_DA_BC",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1005,
    "para_id": 165,
    "paper_id": 1,
    "2d_coord": [
      -1.632767677307129,
      -0.2959460914134979
    ],
    "MSU_id": 1005,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "inventory we used here. Besides, the predictions produced by WRF- Chem_DA_BC revealed better consistency with observations in BTH and YRD. As such, although the predictions were improved in the WRF- Chem_DA_BC experiment, some overestimation errors still existed, with the regional average concentration being 58.84 and  $56.86\\mu \\mathrm{g} / \\mathrm{m}^3$  for the prediction and observations, respectively. In the current DA experiment, we only considered the adjustment of the initial state of the aerosol fields, without giving concern to the adjustment of emissions, which might have limited the improvement in results."
  },
  {
    "sentence": "Despite the superior performance produced by combining DA and BC, certain aspects still need further discussion.",
    "subtitle": "4.1. Discussion",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1006,
    "para_id": 166,
    "paper_id": 1,
    "2d_coord": [
      -1.8100453615188599,
      -1.650524377822876
    ],
    "MSU_id": 1006,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Despite the superior performance produced by combining DA and BC, certain aspects still need further discussion. In this study, we only considered the effect of DA through adjustment of the initial aerosol field, which may have had limitations on the optimization effects. The adjustment of emissions has been proved to be critical in terms of the improvement of aerosol predictions in previous studies (Henze et al., 2009; Ku and Park, 2011). As such, greater optimization might be achievable by the additional assimilation of air quality data through applying an inverse method to retrieve an optimized emission inventory based on the model configuration as in the WRF- Chem_DA_BC experiment, which will be analyzed in future work."
  },
  {
    "sentence": "BC is not sensitive to the length of the forecast.",
    "subtitle": "4.1. Discussion",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1015,
    "para_id": 168,
    "paper_id": 1,
    "2d_coord": [
      -3.2469842433929443,
      -0.6436163187026978
    ],
    "MSU_id": 1015,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Lastly, it should be acknowledged that we mainly compared and analyzed the results of the different experiments in a 0- 24- h forecasting period. Since BC is not sensitive to the length of the forecast, long- term predictions can be analyzed in future work. Also, since many methods have been used in the bias correction of numerical models, work will be carried out with other models in the future to make further comparisons with the results reported here."
  },
  {
    "sentence": "Few methods have considered the combination of DA adjustment and post-processing correction.",
    "subtitle": "4.2. Summary",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1020,
    "para_id": 169,
    "paper_id": 1,
    "2d_coord": [
      -1.608140230178833,
      -1.1098192930221558
    ],
    "MSU_id": 1020,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "Although many methods have been applied in the adjustment of the initial field and output field to improve  $\\mathrm{PM}{2.5}$  concentration predictions, few have considered the combination of DA adjustment and post- processing correction. In this study, the optimization effects of DA in the adjustment of the initial field, and of BC in post- processing, were compared and a new scheme that combined DA and BC simultaneously was developed, showing great advantages in the improvement of  $\\mathrm{PM}{2.5}$  concentration predictions. Four parallel experiments were conducted during winter 2019, including a control experiment directly forecasted by WRF- Chem, an experiment that assimilated in situ observations based on GSI, an experiment with BC based on MDS- UNet, and an experiment that combined DA and BC."
  },
  {
    "sentence": "WRF-Chem_BC_DA still underestimated the PM2.5 concentrations during a pollution process in February 2019.",
    "subtitle": "4.2. Summary",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1056,
    "para_id": 173,
    "paper_id": 1,
    "2d_coord": [
      -2.6831727027893066,
      0.1876758337020874
    ],
    "MSU_id": 1056,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "The averaged  $\\mathrm{PM}{2.5}$  distributions in winter revealed that  $\\mathrm{PM}{2.5}$  predictions were notably overestimated over YRD and CC and around SCB in the control experiment, whereas the WRF- Chem_DA_BC experiment adjusted the concentrations in these regions. The concentrations averaged over BTH, YRD, CC, and SBC were 54.53, 55.93, 60.34, and 41.93  $\\mu \\mathrm{g} / \\mathrm{m}^3$  respectively, which were close to the values in other studies. WRF- Chem_BC_DA better captured particular pollution episodes, but it still underestimated the  $\\mathrm{PM}_{2.5}$  concentrations during a pollution process in February 2019, possibly because of the fixed emissions inventory we used here. In the current DA experiment, we only considered adjustment of the initial state of the aerosol fields, without giving concern to the adjustment of emissions, which might have limited the improvement in results."
  },
  {
    "sentence": "The underestimation was possibly because of the fixed emissions inventory used.",
    "subtitle": "4.2. Summary",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1057,
    "para_id": 173,
    "paper_id": 1,
    "2d_coord": [
      -0.8267587423324585,
      0.29499515891075134
    ],
    "MSU_id": 1057,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "The averaged  $\\mathrm{PM}{2.5}$  distributions in winter revealed that  $\\mathrm{PM}{2.5}$  predictions were notably overestimated over YRD and CC and around SCB in the control experiment, whereas the WRF- Chem_DA_BC experiment adjusted the concentrations in these regions. The concentrations averaged over BTH, YRD, CC, and SBC were 54.53, 55.93, 60.34, and 41.93  $\\mu \\mathrm{g} / \\mathrm{m}^3$  respectively, which were close to the values in other studies. WRF- Chem_BC_DA better captured particular pollution episodes, but it still underestimated the  $\\mathrm{PM}_{2.5}$  concentrations during a pollution process in February 2019, possibly because of the fixed emissions inventory we used here. In the current DA experiment, we only considered adjustment of the initial state of the aerosol fields, without giving concern to the adjustment of emissions, which might have limited the improvement in results."
  },
  {
    "sentence": "The DA experiment did not give concern to the adjustment of emissions.",
    "subtitle": "4.2. Summary",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1059,
    "para_id": 173,
    "paper_id": 1,
    "2d_coord": [
      -0.9783521890640259,
      -0.29592472314834595
    ],
    "MSU_id": 1059,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "The averaged  $\\mathrm{PM}{2.5}$  distributions in winter revealed that  $\\mathrm{PM}{2.5}$  predictions were notably overestimated over YRD and CC and around SCB in the control experiment, whereas the WRF- Chem_DA_BC experiment adjusted the concentrations in these regions. The concentrations averaged over BTH, YRD, CC, and SBC were 54.53, 55.93, 60.34, and 41.93  $\\mu \\mathrm{g} / \\mathrm{m}^3$  respectively, which were close to the values in other studies. WRF- Chem_BC_DA better captured particular pollution episodes, but it still underestimated the  $\\mathrm{PM}_{2.5}$  concentrations during a pollution process in February 2019, possibly because of the fixed emissions inventory we used here. In the current DA experiment, we only considered adjustment of the initial state of the aerosol fields, without giving concern to the adjustment of emissions, which might have limited the improvement in results."
  },
  {
    "sentence": "The lack of emissions adjustment might have limited the improvement in results.",
    "subtitle": "4.2. Summary",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1060,
    "para_id": 173,
    "paper_id": 1,
    "2d_coord": [
      -2.0500295162200928,
      0.3277307152748108
    ],
    "MSU_id": 1060,
    "paper_info": "Improving WRF-Chem PM2.5 predictions by combining data assimilation and deep-learning-based bias correction",
    "paragraph_info": "The averaged  $\\mathrm{PM}{2.5}$  distributions in winter revealed that  $\\mathrm{PM}{2.5}$  predictions were notably overestimated over YRD and CC and around SCB in the control experiment, whereas the WRF- Chem_DA_BC experiment adjusted the concentrations in these regions. The concentrations averaged over BTH, YRD, CC, and SBC were 54.53, 55.93, 60.34, and 41.93  $\\mu \\mathrm{g} / \\mathrm{m}^3$  respectively, which were close to the values in other studies. WRF- Chem_BC_DA better captured particular pollution episodes, but it still underestimated the  $\\mathrm{PM}_{2.5}$  concentrations during a pollution process in February 2019, possibly because of the fixed emissions inventory we used here. In the current DA experiment, we only considered adjustment of the initial state of the aerosol fields, without giving concern to the adjustment of emissions, which might have limited the improvement in results."
  },
  {
    "sentence": "Models have shown substantial discrepancies and errors regarding BBA absorption.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1081,
    "para_id": 176,
    "paper_id": 2,
    "2d_coord": [
      -3.0177621841430664,
      -0.19852817058563232
    ],
    "MSU_id": 1081,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Despite the recent progress in airborne measurements of BBA, large- scale and long- term assessments of the climate impacts of BBA have far relied on global aerosol models (14, 15). However, models have shown substantial discrepancies and errors regarding BBA absorption (16- 19), which remains a key obstacle to reliable climate assessments because BBA absorption can simultaneously contribute to direct, semidirect, and indirect aerosol effects (9, 20). Comparisons between models and satellite observations suggest that global models generally underestimate the overall warming impacts of BBA, as indicated by a substantial underestimation of the aerosol absorption optical depth (AAOD) (21) in combination with an overestimation of the single- scattering albedo (SSA) (20). These findings are supported by flight campaign measurements (22, 23) such as the ObseRvations of Aerosols above CLOUDs and their intEractionS (ORACLES) project (22), suggesting that the overall warming caused by BBA plumes is greater than previously considered, particularly in tropical regions. However, when considering the aerosol composition, global models tend to produce stronger absorption than observed from field and laboratory measurements for a given BC mass mixing ratio (24). The contradiction between the underestimated overall warming and overestimated absorbing capability per unit of BC mass highlights important errors in the emission, composition, and optical properties of absorbing BBA. Although there have been discussions regarding the possible reasons for these model errors [e.g., particle size distribution, vertical profiles, mixing states, and refractive index (24- 28)], a quantitative evaluation of how these factors contribute to the overall errors is still missing. This poses a fundamental challenge to the aerosol modeling community, which hinders reliable climate assessments over tropical regions."
  },
  {
    "sentence": "Comparisons between models and satellite observations suggest that global models generally underestimate the overall warming impacts of BBA.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1084,
    "para_id": 176,
    "paper_id": 2,
    "2d_coord": [
      -2.814767837524414,
      -0.25133103132247925
    ],
    "MSU_id": 1084,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Despite the recent progress in airborne measurements of BBA, large- scale and long- term assessments of the climate impacts of BBA have far relied on global aerosol models (14, 15). However, models have shown substantial discrepancies and errors regarding BBA absorption (16- 19), which remains a key obstacle to reliable climate assessments because BBA absorption can simultaneously contribute to direct, semidirect, and indirect aerosol effects (9, 20). Comparisons between models and satellite observations suggest that global models generally underestimate the overall warming impacts of BBA, as indicated by a substantial underestimation of the aerosol absorption optical depth (AAOD) (21) in combination with an overestimation of the single- scattering albedo (SSA) (20). These findings are supported by flight campaign measurements (22, 23) such as the ObseRvations of Aerosols above CLOUDs and their intEractionS (ORACLES) project (22), suggesting that the overall warming caused by BBA plumes is greater than previously considered, particularly in tropical regions. However, when considering the aerosol composition, global models tend to produce stronger absorption than observed from field and laboratory measurements for a given BC mass mixing ratio (24). The contradiction between the underestimated overall warming and overestimated absorbing capability per unit of BC mass highlights important errors in the emission, composition, and optical properties of absorbing BBA. Although there have been discussions regarding the possible reasons for these model errors [e.g., particle size distribution, vertical profiles, mixing states, and refractive index (24- 28)], a quantitative evaluation of how these factors contribute to the overall errors is still missing. This poses a fundamental challenge to the aerosol modeling community, which hinders reliable climate assessments over tropical regions."
  },
  {
    "sentence": "Global models substantially underestimate the aerosol absorption optical depth (AAOD).",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1085,
    "para_id": 176,
    "paper_id": 2,
    "2d_coord": [
      -1.7903412580490112,
      0.7832318544387817
    ],
    "MSU_id": 1085,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Despite the recent progress in airborne measurements of BBA, large- scale and long- term assessments of the climate impacts of BBA have far relied on global aerosol models (14, 15). However, models have shown substantial discrepancies and errors regarding BBA absorption (16- 19), which remains a key obstacle to reliable climate assessments because BBA absorption can simultaneously contribute to direct, semidirect, and indirect aerosol effects (9, 20). Comparisons between models and satellite observations suggest that global models generally underestimate the overall warming impacts of BBA, as indicated by a substantial underestimation of the aerosol absorption optical depth (AAOD) (21) in combination with an overestimation of the single- scattering albedo (SSA) (20). These findings are supported by flight campaign measurements (22, 23) such as the ObseRvations of Aerosols above CLOUDs and their intEractionS (ORACLES) project (22), suggesting that the overall warming caused by BBA plumes is greater than previously considered, particularly in tropical regions. However, when considering the aerosol composition, global models tend to produce stronger absorption than observed from field and laboratory measurements for a given BC mass mixing ratio (24). The contradiction between the underestimated overall warming and overestimated absorbing capability per unit of BC mass highlights important errors in the emission, composition, and optical properties of absorbing BBA. Although there have been discussions regarding the possible reasons for these model errors [e.g., particle size distribution, vertical profiles, mixing states, and refractive index (24- 28)], a quantitative evaluation of how these factors contribute to the overall errors is still missing. This poses a fundamental challenge to the aerosol modeling community, which hinders reliable climate assessments over tropical regions."
  },
  {
    "sentence": "Global models overestimate the single-scattering albedo (SSA).",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1086,
    "para_id": 176,
    "paper_id": 2,
    "2d_coord": [
      -1.8028709888458252,
      0.22375091910362244
    ],
    "MSU_id": 1086,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Despite the recent progress in airborne measurements of BBA, large- scale and long- term assessments of the climate impacts of BBA have far relied on global aerosol models (14, 15). However, models have shown substantial discrepancies and errors regarding BBA absorption (16- 19), which remains a key obstacle to reliable climate assessments because BBA absorption can simultaneously contribute to direct, semidirect, and indirect aerosol effects (9, 20). Comparisons between models and satellite observations suggest that global models generally underestimate the overall warming impacts of BBA, as indicated by a substantial underestimation of the aerosol absorption optical depth (AAOD) (21) in combination with an overestimation of the single- scattering albedo (SSA) (20). These findings are supported by flight campaign measurements (22, 23) such as the ObseRvations of Aerosols above CLOUDs and their intEractionS (ORACLES) project (22), suggesting that the overall warming caused by BBA plumes is greater than previously considered, particularly in tropical regions. However, when considering the aerosol composition, global models tend to produce stronger absorption than observed from field and laboratory measurements for a given BC mass mixing ratio (24). The contradiction between the underestimated overall warming and overestimated absorbing capability per unit of BC mass highlights important errors in the emission, composition, and optical properties of absorbing BBA. Although there have been discussions regarding the possible reasons for these model errors [e.g., particle size distribution, vertical profiles, mixing states, and refractive index (24- 28)], a quantitative evaluation of how these factors contribute to the overall errors is still missing. This poses a fundamental challenge to the aerosol modeling community, which hinders reliable climate assessments over tropical regions."
  },
  {
    "sentence": "Global models tend to produce stronger absorption than observed from field and laboratory measurements for a given BC mass mixing ratio.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1089,
    "para_id": 176,
    "paper_id": 2,
    "2d_coord": [
      -1.8341519832611084,
      0.8947017192840576
    ],
    "MSU_id": 1089,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Despite the recent progress in airborne measurements of BBA, large- scale and long- term assessments of the climate impacts of BBA have far relied on global aerosol models (14, 15). However, models have shown substantial discrepancies and errors regarding BBA absorption (16- 19), which remains a key obstacle to reliable climate assessments because BBA absorption can simultaneously contribute to direct, semidirect, and indirect aerosol effects (9, 20). Comparisons between models and satellite observations suggest that global models generally underestimate the overall warming impacts of BBA, as indicated by a substantial underestimation of the aerosol absorption optical depth (AAOD) (21) in combination with an overestimation of the single- scattering albedo (SSA) (20). These findings are supported by flight campaign measurements (22, 23) such as the ObseRvations of Aerosols above CLOUDs and their intEractionS (ORACLES) project (22), suggesting that the overall warming caused by BBA plumes is greater than previously considered, particularly in tropical regions. However, when considering the aerosol composition, global models tend to produce stronger absorption than observed from field and laboratory measurements for a given BC mass mixing ratio (24). The contradiction between the underestimated overall warming and overestimated absorbing capability per unit of BC mass highlights important errors in the emission, composition, and optical properties of absorbing BBA. Although there have been discussions regarding the possible reasons for these model errors [e.g., particle size distribution, vertical profiles, mixing states, and refractive index (24- 28)], a quantitative evaluation of how these factors contribute to the overall errors is still missing. This poses a fundamental challenge to the aerosol modeling community, which hinders reliable climate assessments over tropical regions."
  },
  {
    "sentence": "MAC has been identified as an important but highly uncertain parameter.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1104,
    "para_id": 179,
    "paper_id": 2,
    "2d_coord": [
      -1.2834259271621704,
      -0.8739621639251709
    ],
    "MSU_id": 1104,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "OA) emissions, aerosol lifetime, and mass absorption coefficient (MAC), respectively. Other absorbing components (e.g., dust) account for only  $4\\%$  of AAOD in the model ensemble over the two focused regions and are therefore ignored. Because of the definitions of aerosol lifetime (column burden of  $\\mathrm{BC + OA}$  divided by  $E$  and MAC (AAOD divided by the column burden of  $\\mathrm{BC + OA}$  this equation is always applicable. Both lifetime and MAC are emergent properties of models, resulting from various physical and chemical processes. MAC has been identified as an important but highly uncertain parameter (29) and exhibits a wide range across models (19).Note that we consider secondary OA (SOA) formation as part of the total emissions given the short formation time scale compared to the seasonal time scale in our analysis (30)."
  },
  {
    "sentence": "MAC exhibits a wide range across models.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1105,
    "para_id": 179,
    "paper_id": 2,
    "2d_coord": [
      -3.2204198837280273,
      -0.3784308135509491
    ],
    "MSU_id": 1105,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "OA) emissions, aerosol lifetime, and mass absorption coefficient (MAC), respectively. Other absorbing components (e.g., dust) account for only  $4\\%$  of AAOD in the model ensemble over the two focused regions and are therefore ignored. Because of the definitions of aerosol lifetime (column burden of  $\\mathrm{BC + OA}$  divided by  $E$  and MAC (AAOD divided by the column burden of  $\\mathrm{BC + OA}$  this equation is always applicable. Both lifetime and MAC are emergent properties of models, resulting from various physical and chemical processes. MAC has been identified as an important but highly uncertain parameter (29) and exhibits a wide range across models (19).Note that we consider secondary OA (SOA) formation as part of the total emissions given the short formation time scale compared to the seasonal time scale in our analysis (30)."
  },
  {
    "sentence": "The constrained results are only effective on a regional and seasonal scale.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1117,
    "para_id": 180,
    "paper_id": 2,
    "2d_coord": [
      -3.205672025680542,
      -0.2188829630613327
    ],
    "MSU_id": 1117,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "The overall procedure for constraining the above three components is somewhat similar to strategies commonly used in the context of \"emergent constraints,\" but we have introduced a closure relation (based on a simple box model) that allows estimating three components from two modeled relationships. It is similar to the methodology applied in a previous study (31), in which we analyze AOD and considered all aerosols instead of carbonaceous aerosols only. Briefly, we linearly regressed modeled MAC against modeled SSA and modeled  $\\tau$  against modeled precipitation and the angstrom exponent (AE; an indicator of ambient particle size) using the model data from the AeroCom (Aerosol Comparisons between Observations and Models; see table S1) project. Then, we applied satellite observations of SSA to estimate the constrained MAC and similarly constrain  $\\tau$  from observations of precipitation and AE. Last, we used Eq. 1 to constrain  $E$  The constrained values of  $E,\\tau ,$  and MAC allow us to attribute AAOD errors to contributions from these three factors for individual models. Uncertainty analysis is conducted for all these constraining processes as shown in Fig. 1. Notably, the constrained results are only effective on a regional and seasonal scale, and caution must be exercised when directly applying these results to smaller scales. This work presents advancements upon the foundation of (31) as it constrains MAC instead of MEC (mass extinction coefficient). In addition, we implement in situ data in the interpretation of satellite observations that allows a disaggregation of BC and OC emissions. Furthermore, in our seasonal, regional analysis, we find that SOA formation is important for fire aerosols over the Amazon but not over Africa."
  },
  {
    "sentence": "Caution must be exercised when directly applying these results to smaller scales.",
    "subtitle": "INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1118,
    "para_id": 180,
    "paper_id": 2,
    "2d_coord": [
      -3.0261764526367188,
      -0.907927393913269
    ],
    "MSU_id": 1118,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "The overall procedure for constraining the above three components is somewhat similar to strategies commonly used in the context of \"emergent constraints,\" but we have introduced a closure relation (based on a simple box model) that allows estimating three components from two modeled relationships. It is similar to the methodology applied in a previous study (31), in which we analyze AOD and considered all aerosols instead of carbonaceous aerosols only. Briefly, we linearly regressed modeled MAC against modeled SSA and modeled  $\\tau$  against modeled precipitation and the angstrom exponent (AE; an indicator of ambient particle size) using the model data from the AeroCom (Aerosol Comparisons between Observations and Models; see table S1) project. Then, we applied satellite observations of SSA to estimate the constrained MAC and similarly constrain  $\\tau$  from observations of precipitation and AE. Last, we used Eq. 1 to constrain  $E$  The constrained values of  $E,\\tau ,$  and MAC allow us to attribute AAOD errors to contributions from these three factors for individual models. Uncertainty analysis is conducted for all these constraining processes as shown in Fig. 1. Notably, the constrained results are only effective on a regional and seasonal scale, and caution must be exercised when directly applying these results to smaller scales. This work presents advancements upon the foundation of (31) as it constrains MAC instead of MEC (mass extinction coefficient). In addition, we implement in situ data in the interpretation of satellite observations that allows a disaggregation of BC and OC emissions. Furthermore, in our seasonal, regional analysis, we find that SOA formation is important for fire aerosols over the Amazon but not over Africa."
  },
  {
    "sentence": "The analysis highlights errors in total emissions.",
    "subtitle": "BC and OA emissions in BB regions",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1162,
    "para_id": 188,
    "paper_id": 2,
    "2d_coord": [
      0.2628689110279083,
      -0.5360996723175049
    ],
    "MSU_id": 1162,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Although the above analysis highlights the errors in total emissions, it can provide more valuable insights if the total emissions can be speciated into either BC or OA emissions with a constrained rBC [i.e.,  $\\mathrm{BC:(\\bar{B}C + OA)}$ ]. To achieve this goal, we use the linear relationship observed by (24) between rBC and SSA to estimate ambient rBC from satellite observations of SSA. AeroCom models allow us to establish a relationship between ambient and emitted rBC, which, in any case, is close to identical (see Materials and Methods). These constrained BC and OA emissions can be compared with four widely used inventories of BC and OC emissions [i.e., Global Fire Emission Database version 4.1s (GFED), Global Fire Assimilation System version 1.2 (GFAS), Fire Energetics and Emission Research version 1.2 (FEER), and Quick Fire Emissions Dataset version 2.5 (QFED)], providing that we have reasonable OA/OC ratios (Fig. 4). Field measurements of primary BBA emissions exhibit a narrow range (1.5 to 1.9) for OA/OC (33- 39). We find that most inventories estimate lower emissions than our estimates over Africa, which partly explains the negative AAOD bias (Fig. 3B), as these inventories are used in AeroCom models (table S1)."
  },
  {
    "sentence": "Inventories with underestimated emissions may be related to undetected small fires.",
    "subtitle": "BC and OA emissions in BB regions",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1175,
    "para_id": 189,
    "paper_id": 2,
    "2d_coord": [
      0.4801027774810791,
      -0.018355272710323334
    ],
    "MSU_id": 1175,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Inventories with underestimated emissions may be related to undetected small fires (40). The discrepancy is also affected by the variation in emission factor (EF), but it is unlikely to fully explain the low emissions, as the EFs used in the inventories generally agree with field measurements (see table S2 and fig. S8). In the Amazon, the inventories also estimate lower OA emission levels relative to our constrained OA. However, the BC inventory emissions are generally higher (except in the GFAS inventory), which is consistent with the overestimated MAC in models (Fig. 3A). Again, this cannot be explained by uncertainties in the EFs (fig. S8), which implies that the low OA emissions in the inventories over the Amazon result from missing sources."
  },
  {
    "sentence": "The discrepancy in emissions is affected by the variation in emission factor (EF).",
    "subtitle": "BC and OA emissions in BB regions",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 1176,
    "para_id": 189,
    "paper_id": 2,
    "2d_coord": [
      -2.241636276245117,
      0.8094614744186401
    ],
    "MSU_id": 1176,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Inventories with underestimated emissions may be related to undetected small fires (40). The discrepancy is also affected by the variation in emission factor (EF), but it is unlikely to fully explain the low emissions, as the EFs used in the inventories generally agree with field measurements (see table S2 and fig. S8). In the Amazon, the inventories also estimate lower OA emission levels relative to our constrained OA. However, the BC inventory emissions are generally higher (except in the GFAS inventory), which is consistent with the overestimated MAC in models (Fig. 3A). Again, this cannot be explained by uncertainties in the EFs (fig. S8), which implies that the low OA emissions in the inventories over the Amazon result from missing sources."
  },
  {
    "sentence": "The variation in emission factor is unlikely to fully explain the low emissions.",
    "subtitle": "BC and OA emissions in BB regions",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1177,
    "para_id": 189,
    "paper_id": 2,
    "2d_coord": [
      -1.874881625175476,
      0.09694400429725647
    ],
    "MSU_id": 1177,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Inventories with underestimated emissions may be related to undetected small fires (40). The discrepancy is also affected by the variation in emission factor (EF), but it is unlikely to fully explain the low emissions, as the EFs used in the inventories generally agree with field measurements (see table S2 and fig. S8). In the Amazon, the inventories also estimate lower OA emission levels relative to our constrained OA. However, the BC inventory emissions are generally higher (except in the GFAS inventory), which is consistent with the overestimated MAC in models (Fig. 3A). Again, this cannot be explained by uncertainties in the EFs (fig. S8), which implies that the low OA emissions in the inventories over the Amazon result from missing sources."
  },
  {
    "sentence": "The OA/OC ratios used by AeroCom models tend to spread larger than the aforementioned field measurements.",
    "subtitle": "BC and OA emissions in BB regions",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1185,
    "para_id": 190,
    "paper_id": 2,
    "2d_coord": [
      -1.9293712377548218,
      -0.7089031934738159
    ],
    "MSU_id": 1185,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "It is important to mention that the AeroCom models also apply OA/OC ratios to estimate OA emissions from the inventories. However, their OA/OC ratios (1.4 to 2.6; see table S1) tend to spread larger than the aforementioned field measurements, suggesting that part of the emission errors shown in Fig. 3 are due to inappropriate OA/OC ratios."
  },
  {
    "sentence": "Part of the emission errors shown in Fig. 3 are due to inappropriate OA/OC ratios.",
    "subtitle": "BC and OA emissions in BB regions",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1186,
    "para_id": 190,
    "paper_id": 2,
    "2d_coord": [
      -1.7702020406723022,
      0.4515347182750702
    ],
    "MSU_id": 1186,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "It is important to mention that the AeroCom models also apply OA/OC ratios to estimate OA emissions from the inventories. However, their OA/OC ratios (1.4 to 2.6; see table S1) tend to spread larger than the aforementioned field measurements, suggesting that part of the emission errors shown in Fig. 3 are due to inappropriate OA/OC ratios."
  },
  {
    "sentence": "Satellite observations inherently have a considerable retrieval uncertainty in AAOD data.",
    "subtitle": "Correcting AAOD errors in two global models",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 1210,
    "para_id": 195,
    "paper_id": 2,
    "2d_coord": [
      -2.228935956954956,
      -0.2949041724205017
    ],
    "MSU_id": 1210,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "The above error analysis is used to improve two global models by correcting the identified errors in the three components (Materials and Methods and table S3). The ECHAM- HAM [a model developed from the atmospheric model by European Center for Medium- Range Weather Forecasts (EC) and a parameterization package developed at Hamburg (HAM)] and SPRINTARS (Spectral Radiation- Transport Model for Aerosol Species) models are selected given their opposite SSA errors in the AeroCom ensemble (fig. S6). The models have been thoroughly validated against satellite observations, which inherently have a considerable retrieval uncertainty in AAOD data. However, it is worth noting that this retrieval uncertainty is generally smaller than the default errors in the model. As shown in Fig. 5, our corrections over the BBA source regions in both models have reduced the seasonal AAOD error, especially in the ECHAM- HAM model. Although the SPRINTARS AAOD error has slightly increased following the corrections, it remains comparable to the uncertainty associated with satellite retrievals. The SSA is also found to better agree with satellite observations, suggesting the robustness of our analysis and corrections. In addition, the modeled AOD exhibits much smaller errors, which independently verifies our analysis. Moreover, our corrections over the source regions also benefit the simulations in the outflow areas, with smaller errors found for AAOD, AOD, and SSA in the two models (figs. S10 and S11). This model improvement is also found when validated against independent Aerosol Robotic Network (AERONET) observations (fig. S12)."
  },
  {
    "sentence": "The retrieval uncertainty is generally smaller than the default errors in the model.",
    "subtitle": "Correcting AAOD errors in two global models",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1211,
    "para_id": 195,
    "paper_id": 2,
    "2d_coord": [
      -3.0273349285125732,
      -1.4664278030395508
    ],
    "MSU_id": 1211,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "The above error analysis is used to improve two global models by correcting the identified errors in the three components (Materials and Methods and table S3). The ECHAM- HAM [a model developed from the atmospheric model by European Center for Medium- Range Weather Forecasts (EC) and a parameterization package developed at Hamburg (HAM)] and SPRINTARS (Spectral Radiation- Transport Model for Aerosol Species) models are selected given their opposite SSA errors in the AeroCom ensemble (fig. S6). The models have been thoroughly validated against satellite observations, which inherently have a considerable retrieval uncertainty in AAOD data. However, it is worth noting that this retrieval uncertainty is generally smaller than the default errors in the model. As shown in Fig. 5, our corrections over the BBA source regions in both models have reduced the seasonal AAOD error, especially in the ECHAM- HAM model. Although the SPRINTARS AAOD error has slightly increased following the corrections, it remains comparable to the uncertainty associated with satellite retrievals. The SSA is also found to better agree with satellite observations, suggesting the robustness of our analysis and corrections. In addition, the modeled AOD exhibits much smaller errors, which independently verifies our analysis. Moreover, our corrections over the source regions also benefit the simulations in the outflow areas, with smaller errors found for AAOD, AOD, and SSA in the two models (figs. S10 and S11). This model improvement is also found when validated against independent Aerosol Robotic Network (AERONET) observations (fig. S12)."
  },
  {
    "sentence": "Model estimates of SOA production are very uncertain and usually fail to show regional differences between the Amazon and Africa.",
    "subtitle": "DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1247,
    "para_id": 199,
    "paper_id": 2,
    "2d_coord": [
      -2.831359386444092,
      -0.034684307873249054
    ],
    "MSU_id": 1247,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "In addition to primary BB emissions, our analysis suggests that substantial SOA formation contributes to the modeled aerosol absorption errors over the Amazon but not over Africa. The remarkable regional difference may be associated with the variation in precursor gas emissions from biogenic sources, with a notably higher level of such emissions found over the Amazon than over Africa (fig. S14). In comparison, model estimates of SOA production are very uncertain and usually fail to show regional differences between the Amazon and Africa. Our estimation, to our knowledge, is the first one in which SOA formation is constrained by aerosol absorption, which provides an innovative perspective on SOA estimation. The results also highlight the importance of SOA during fire seasons. This particular aspect has often been overlooked, and our findings emphasize the need for further research."
  },
  {
    "sentence": "The importance of SOA during fire seasons has often been overlooked.",
    "subtitle": "DISCUSSION",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 1251,
    "para_id": 199,
    "paper_id": 2,
    "2d_coord": [
      0.18641521036624908,
      0.09031671285629272
    ],
    "MSU_id": 1251,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "In addition to primary BB emissions, our analysis suggests that substantial SOA formation contributes to the modeled aerosol absorption errors over the Amazon but not over Africa. The remarkable regional difference may be associated with the variation in precursor gas emissions from biogenic sources, with a notably higher level of such emissions found over the Amazon than over Africa (fig. S14). In comparison, model estimates of SOA production are very uncertain and usually fail to show regional differences between the Amazon and Africa. Our estimation, to our knowledge, is the first one in which SOA formation is constrained by aerosol absorption, which provides an innovative perspective on SOA estimation. The results also highlight the importance of SOA during fire seasons. This particular aspect has often been overlooked, and our findings emphasize the need for further research."
  },
  {
    "sentence": "Previous studies have suggested large and diverse errors in the satellite-based observations of AAOD and SSA.",
    "subtitle": "Observation data",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1284,
    "para_id": 204,
    "paper_id": 2,
    "2d_coord": [
      -0.49140554666519165,
      0.24086037278175354
    ],
    "MSU_id": 1284,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Previous studies have suggested large and diverse errors in the satellite- based observations of AAOD and SSA (47). To characterize satellite errors, we consider three datasets: the Polarization and Directionality of the Earth's Reflectances with the Generalized Retrieval of Aerosol and Surface Properties (POLDER- GRASP) algorithm, the Ozone Monitoring Instrument (OMI) with UV aerosol algorithm (OMAERUV), and the Advanced Along- Track Scanning Radiometer with Optimal Retrieval of Aerosol and Cloud algorithm (AATSR- ORAC). We validate the three datasets against AERONET data (the locations of the AERONET sites used in this study are shown in fig. S1) following the procedure in (47) and find that POLDER- GRASP exhibits the lowest error and has the highest correlations with AERONET data for both AAOD and SSA (fig. S16). Therefore, we use POLDER- GRASP as the satellite observation (including AE) during 2010 fire seasons throughout the analysis. However, even for POLDER- GRASP, large retrieval errors exist, which contribute substantially to the overall uncertainties presented in this work."
  },
  {
    "sentence": "Even for POLDER-GRASP, large retrieval errors exist, which contribute substantially to the overall uncertainties presented in this work.",
    "subtitle": "Observation data",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1290,
    "para_id": 204,
    "paper_id": 2,
    "2d_coord": [
      -3.005582332611084,
      -0.6642359495162964
    ],
    "MSU_id": 1290,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Previous studies have suggested large and diverse errors in the satellite- based observations of AAOD and SSA (47). To characterize satellite errors, we consider three datasets: the Polarization and Directionality of the Earth's Reflectances with the Generalized Retrieval of Aerosol and Surface Properties (POLDER- GRASP) algorithm, the Ozone Monitoring Instrument (OMI) with UV aerosol algorithm (OMAERUV), and the Advanced Along- Track Scanning Radiometer with Optimal Retrieval of Aerosol and Cloud algorithm (AATSR- ORAC). We validate the three datasets against AERONET data (the locations of the AERONET sites used in this study are shown in fig. S1) following the procedure in (47) and find that POLDER- GRASP exhibits the lowest error and has the highest correlations with AERONET data for both AAOD and SSA (fig. S16). Therefore, we use POLDER- GRASP as the satellite observation (including AE) during 2010 fire seasons throughout the analysis. However, even for POLDER- GRASP, large retrieval errors exist, which contribute substantially to the overall uncertainties presented in this work."
  },
  {
    "sentence": "Regional estimation of AAOD cannot be obtained directly from the sparsely sampled raw satellite data.",
    "subtitle": "Observation data",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1292,
    "para_id": 205,
    "paper_id": 2,
    "2d_coord": [
      -2.677462100982666,
      -0.42582574486732483
    ],
    "MSU_id": 1292,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "To apply Eq. 1 in our analysis, a regional estimation of AAOD is required, which cannot be obtained directly from the sparsely sampled raw satellite data. Following the homogenization method in our previous work (31), we perform a linear regression between the modeled regional AAOD and modeled AAOD with POLDER- GRASP sampling (averaged over the region and fire season; fig. S17). The raw POLDER- GRASP data are then applied to the regression to estimate the regional AAOD. A similar method is also used to estimate the regional SSA. The robustness of the method is verified through a jackknife test by removing the models one by one, which produces small relative variations in the predicted regional AAOD and SSA (<1%), suggesting that the construction of regional values is independent of the models used. The regional observations of AAOD and SSA are also used to validate the models on a seasonal scale (see fig. S18), showing a varying degree of error per model. Broadly, models tend to underestimate SSA over the Amazon (by 0.05 on average) and underestimate AAOD over Southern Africa (by 32% on average)."
  },
  {
    "sentence": "We do not use the modeled relationship between rBC and SSA, as we find that it contains a large error.",
    "subtitle": "Constraining BB carbonaceous aerosols in models",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1331,
    "para_id": 211,
    "paper_id": 2,
    "2d_coord": [
      -0.48163604736328125,
      -0.7456297874450684
    ],
    "MSU_id": 1331,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "In addition to the total emissions for  $\\mathrm{BC + OA}$ , the rBC ratio is constrained in this work by using the observational relationship between SSA and ambient rBC at  $550~\\mathrm{nm}$  wavelength from (24). Here, we do not use the modeled relationship between rBC and SSA, as we find that it contains a large error (see fig. S6). It should be noted that the relationship by (24) was established under the criteria that OA and BC accounted for more than  $85\\%$  of the total aerosol mass (to focus on BBAs). For the AeroCom models, we compare the SSA for total aerosols and the SSA of grid cells with  $\\geq 85\\%$  carbonaceous aerosol components, with small differences being found for most models, especially within the range of the SSA observations (see fig. S20A). This suggests that the SSA for total aerosols during fire seasons could sufficiently represent the aerosol criteria by (24), which allows us to constrain the rBC in the ambient aerosols from SSA on the basis of the observed relationship. Using another regression to link the rBC from ambient aerosols to the emissions (see fig. S20B), we lastly obtain the constrained rBC in emissions and estimate the separate emissions for BC and OA."
  },
  {
    "sentence": "This value is lower than those used in the AeroCom models (0.44 to 0.79 for the imaginary part).",
    "subtitle": "Global model simulations and corrections",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 1388,
    "para_id": 217,
    "paper_id": 2,
    "2d_coord": [
      -2.5048396587371826,
      0.43899843096733093
    ],
    "MSU_id": 1388,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "1) Particle size. The modeled ambient particle size is modified to match the observed AE. For ECHAM-HAM, we increase the emitted/ambient particle size by referring to our previous study (31). In SPRINTARS, we switch off the hygroscopic growth for OA which is likely too strong (57). Modifications to both models produce better agreement with the AE observations from POLDER-GRASP (fig. S22A). The AE bias in the default simulations and the corresponding improvement through corrections are further supported by AERONET observations (fig. S22B).\n2) Refractive index for BC. According to field measurements (58, 59), the imaginary part of refractive index of BC in the two models is changed to  $0.3i$ . Note that this value is lower than those used in the AeroCom models (0.44 to 0.79 for the imaginary part; see table S1). We conduct a sensitivity test on the imaginary part ranging from 0.1 to 0.5 with the corrected ECHAM-HAM model and find that the modified particle size with a value of 0.3 agrees the best with SSA observations from (24), suggesting that the observation-based refractive index is more suitable than those used in the AeroCom models (see fig. S23). In addition to SSA, this correction also results in better agreement with our constrained MAC for the two models (fig. S24). This refractive index is also used in a previous model study (60). Moreover, we also test different real parts of refractive index by changing values from 1.4 to 1.95 encompassing both observed values (61) and model-recommended values (see table S1). The resulting changes in AAOD are negligible  $(< 1\\%)$  and we maintain the default model values.\n3) Precipitation. A scaling factor is directly added to the modeled wet deposition based on the default precipitation error. This will correct the lifetime together with the modified particle size as stated above.\n4) Emissions. BC and total OA emissions (both primary emissions and those from secondary formation) are scaled to our constrained results."
  },
  {
    "sentence": "The modifications may not be directly applicable to different times or domains.",
    "subtitle": "Global model simulations and corrections",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1404,
    "para_id": 218,
    "paper_id": 2,
    "2d_coord": [
      -3.0995566844940186,
      -0.46331021189689636
    ],
    "MSU_id": 1404,
    "paper_info": "Threefold reduction of modeled uncertainty in direct radiative effects over biomass burning regions by constraining absorbing aerosols",
    "paragraph_info": "Details of the parameterizations can be found in table S3, and the impacts of the modifications on SSA and MAC are shown in fig. S24. The corrected simulations are validated against POLDER- GRASP (Fig. 5 and figs. S10 and S11), showing better agreement than the default simulations. Please note that the above modifications are conducted specifically over the selected regions during fire seasons and may not be directly applicable to different times or domains."
  },
  {
    "sentence": "The above methods, considered as the strategy of direct depiction, are not effective for large-scale ST series.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1436,
    "para_id": 223,
    "paper_id": 4,
    "2d_coord": [
      -2.5683422088623047,
      -0.4828726351261139
    ],
    "MSU_id": 1436,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Traditionally, ST series are first depicted in temporal visualizations, e.g., line charts. These visualizations are then either plotted on a map by their geographic positions [54, 65], or displayed in a separate view that is coordinated with a map [43], and thereby can be related back to the geographic context. The above methods, considered as the strategy of direct depiction [37], are not effective for large- scale ST series. An analyst may find it difficult to browse the spatial distribution in the vast space and diverse, dynamically changing trends over a long time to gain insights (e.g., trends, correlation, and anomalies). Another strategy is to transform ST series by summarizing them into summaries [19,42] (e.g., variations of consecutive records [24]) or by extracting patterns from them [10, 13] (e.g., co- occurrence patterns [23]). These summaries or patterns, rather than massive raw ST series, are then visually exposed. Yet, the trend narrative and dynamics of ST series, the fundamental features of ST series data, are broken into summaries [19] or patterns [9, 23]. Neither patterns nor summaries are depicted over time. Analysts cannot intuitively perceive how the recorded phenomena develop in the geographic space over time. For example, how quickly does air quality in an area deteriorate/improve? How long is the time before it deteriorates again? Is the deteriorating area expanding?"
  },
  {
    "sentence": "The trend narrative and dynamics of ST series, the fundamental features of ST series data, are broken into summaries or patterns.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1440,
    "para_id": 223,
    "paper_id": 4,
    "2d_coord": [
      -0.3063631057739258,
      -1.3872100114822388
    ],
    "MSU_id": 1440,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Traditionally, ST series are first depicted in temporal visualizations, e.g., line charts. These visualizations are then either plotted on a map by their geographic positions [54, 65], or displayed in a separate view that is coordinated with a map [43], and thereby can be related back to the geographic context. The above methods, considered as the strategy of direct depiction [37], are not effective for large- scale ST series. An analyst may find it difficult to browse the spatial distribution in the vast space and diverse, dynamically changing trends over a long time to gain insights (e.g., trends, correlation, and anomalies). Another strategy is to transform ST series by summarizing them into summaries [19,42] (e.g., variations of consecutive records [24]) or by extracting patterns from them [10, 13] (e.g., co- occurrence patterns [23]). These summaries or patterns, rather than massive raw ST series, are then visually exposed. Yet, the trend narrative and dynamics of ST series, the fundamental features of ST series data, are broken into summaries [19] or patterns [9, 23]. Neither patterns nor summaries are depicted over time. Analysts cannot intuitively perceive how the recorded phenomena develop in the geographic space over time. For example, how quickly does air quality in an area deteriorate/improve? How long is the time before it deteriorates again? Is the deteriorating area expanding?"
  },
  {
    "sentence": "Neither patterns nor summaries are depicted over time.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1441,
    "para_id": 223,
    "paper_id": 4,
    "2d_coord": [
      -0.656614363193512,
      -2.2750773429870605
    ],
    "MSU_id": 1441,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Traditionally, ST series are first depicted in temporal visualizations, e.g., line charts. These visualizations are then either plotted on a map by their geographic positions [54, 65], or displayed in a separate view that is coordinated with a map [43], and thereby can be related back to the geographic context. The above methods, considered as the strategy of direct depiction [37], are not effective for large- scale ST series. An analyst may find it difficult to browse the spatial distribution in the vast space and diverse, dynamically changing trends over a long time to gain insights (e.g., trends, correlation, and anomalies). Another strategy is to transform ST series by summarizing them into summaries [19,42] (e.g., variations of consecutive records [24]) or by extracting patterns from them [10, 13] (e.g., co- occurrence patterns [23]). These summaries or patterns, rather than massive raw ST series, are then visually exposed. Yet, the trend narrative and dynamics of ST series, the fundamental features of ST series data, are broken into summaries [19] or patterns [9, 23]. Neither patterns nor summaries are depicted over time. Analysts cannot intuitively perceive how the recorded phenomena develop in the geographic space over time. For example, how quickly does air quality in an area deteriorate/improve? How long is the time before it deteriorates again? Is the deteriorating area expanding?"
  },
  {
    "sentence": "The traditional Storyline can neither display the fine-grained temporal trends nor provide the spatial context for spatiotemporal series analyses.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1470,
    "para_id": 227,
    "paper_id": 4,
    "2d_coord": [
      -0.3011695146560669,
      -1.9549415111541748
    ],
    "MSU_id": 1470,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Presentation of spatiotemporal information. The traditional Storyline can neither display the fine- grained temporal trends nor provide the spatial context for ST series analyses. Previous studies [17, 62] attempted to extend Storyline techniques to spatiotemporal scenarios but cannot accommodate large- scale datasets with many locations and long time ranges. The Storyline occupies the most effective visual channel (i.e., position) and is compact in layout. Thus, popular temporal visualizations (e.g., line charts) and spatial visualizations (e.g., a map) are difficult to integrate."
  },
  {
    "sentence": "Previous studies attempted to extend Storyline techniques to spatiotemporal scenarios.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1471,
    "para_id": 227,
    "paper_id": 4,
    "2d_coord": [
      -0.15758678317070007,
      -2.1144893169403076
    ],
    "MSU_id": 1471,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Presentation of spatiotemporal information. The traditional Storyline can neither display the fine- grained temporal trends nor provide the spatial context for ST series analyses. Previous studies [17, 62] attempted to extend Storyline techniques to spatiotemporal scenarios but cannot accommodate large- scale datasets with many locations and long time ranges. The Storyline occupies the most effective visual channel (i.e., position) and is compact in layout. Thus, popular temporal visualizations (e.g., line charts) and spatial visualizations (e.g., a map) are difficult to integrate."
  },
  {
    "sentence": "Previous studies cannot accommodate large-scale datasets with many locations and long time ranges.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1472,
    "para_id": 227,
    "paper_id": 4,
    "2d_coord": [
      -2.4553041458129883,
      -1.41416335105896
    ],
    "MSU_id": 1472,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Presentation of spatiotemporal information. The traditional Storyline can neither display the fine- grained temporal trends nor provide the spatial context for ST series analyses. Previous studies [17, 62] attempted to extend Storyline techniques to spatiotemporal scenarios but cannot accommodate large- scale datasets with many locations and long time ranges. The Storyline occupies the most effective visual channel (i.e., position) and is compact in layout. Thus, popular temporal visualizations (e.g., line charts) and spatial visualizations (e.g., a map) are difficult to integrate."
  },
  {
    "sentence": "Popular temporal visualizations, such as line charts, and spatial visualizations, such as a map, are difficult to integrate with the Storyline.",
    "subtitle": "1 INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1474,
    "para_id": 227,
    "paper_id": 4,
    "2d_coord": [
      -0.2564934194087982,
      -2.2874491214752197
    ],
    "MSU_id": 1474,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Presentation of spatiotemporal information. The traditional Storyline can neither display the fine- grained temporal trends nor provide the spatial context for ST series analyses. Previous studies [17, 62] attempted to extend Storyline techniques to spatiotemporal scenarios but cannot accommodate large- scale datasets with many locations and long time ranges. The Storyline occupies the most effective visual channel (i.e., position) and is compact in layout. Thus, popular temporal visualizations (e.g., line charts) and spatial visualizations (e.g., a map) are difficult to integrate."
  },
  {
    "sentence": "Existing methods treated spatial and temporal dimensions separately.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1522,
    "para_id": 234,
    "paper_id": 4,
    "2d_coord": [
      -1.375480055809021,
      -1.7140169143676758
    ],
    "MSU_id": 1522,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Summarization. In tradition, ST series are commonly summarized into multiple heatmaps by time, and each heatmap presented the spatial distribution of the recorded readings in a time interval [31, 38, 64]. For example, Meshesha et al. [36] used four heatmaps to depict the distribution of dissolved oxygen in a river over four seasons. Visualization researchers designed visualizations to summarize ST series into more in- depth features [4, 19, 20, 24]. For example, Li et al. [24] derived and visualized variations in recorded climate data to discern climate changes. Clustering can also help summarize ST series [60]. The ST series of the same cluster can be visually summarized by a representative one because they are similar [74]. Yet, existing methods treated spatial and temporal dimensions separately [60] and thus cannot capture the spatiotemporal relationship that shifts in both time and space. Besides, considering the whole time span may ignore local features. To this end, the sliding window strategy is helpful [1, 40]. A window slides along time, and its size is far less than the series' length. Clustering is performed on the parts of time series covered by the window."
  },
  {
    "sentence": "Existing methods cannot capture the spatiotemporal relationship that shifts in both time and space.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1523,
    "para_id": 234,
    "paper_id": 4,
    "2d_coord": [
      -3.1433215141296387,
      -0.9799643754959106
    ],
    "MSU_id": 1523,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Summarization. In tradition, ST series are commonly summarized into multiple heatmaps by time, and each heatmap presented the spatial distribution of the recorded readings in a time interval [31, 38, 64]. For example, Meshesha et al. [36] used four heatmaps to depict the distribution of dissolved oxygen in a river over four seasons. Visualization researchers designed visualizations to summarize ST series into more in- depth features [4, 19, 20, 24]. For example, Li et al. [24] derived and visualized variations in recorded climate data to discern climate changes. Clustering can also help summarize ST series [60]. The ST series of the same cluster can be visually summarized by a representative one because they are similar [74]. Yet, existing methods treated spatial and temporal dimensions separately [60] and thus cannot capture the spatiotemporal relationship that shifts in both time and space. Besides, considering the whole time span may ignore local features. To this end, the sliding window strategy is helpful [1, 40]. A window slides along time, and its size is far less than the series' length. Clustering is performed on the parts of time series covered by the window."
  },
  {
    "sentence": "Considering the whole time span may ignore local features.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1524,
    "para_id": 234,
    "paper_id": 4,
    "2d_coord": [
      -3.007585048675537,
      -1.1275030374526978
    ],
    "MSU_id": 1524,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Summarization. In tradition, ST series are commonly summarized into multiple heatmaps by time, and each heatmap presented the spatial distribution of the recorded readings in a time interval [31, 38, 64]. For example, Meshesha et al. [36] used four heatmaps to depict the distribution of dissolved oxygen in a river over four seasons. Visualization researchers designed visualizations to summarize ST series into more in- depth features [4, 19, 20, 24]. For example, Li et al. [24] derived and visualized variations in recorded climate data to discern climate changes. Clustering can also help summarize ST series [60]. The ST series of the same cluster can be visually summarized by a representative one because they are similar [74]. Yet, existing methods treated spatial and temporal dimensions separately [60] and thus cannot capture the spatiotemporal relationship that shifts in both time and space. Besides, considering the whole time span may ignore local features. To this end, the sliding window strategy is helpful [1, 40]. A window slides along time, and its size is far less than the series' length. Clustering is performed on the parts of time series covered by the window."
  },
  {
    "sentence": "By means of summarization or pattern extraction of large-scale ST series, analysts may not easily follow the temporal trends and dynamics of ST series between summaries or patterns.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1533,
    "para_id": 236,
    "paper_id": 4,
    "2d_coord": [
      -1.8067307472229004,
      -0.29837605357170105
    ],
    "MSU_id": 1533,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "If large- scale ST series are depicted directly, analysts may find it hard to gain insights into data due to the sheer number of ST series to be investigated. By means of summarization or pattern extraction of large- scale ST series, analysts may not easily follow the temporal trends and dynamics of ST series between summaries or patterns. We define the evolution pattern in ST series and develop a framework to detect the patterns. Visualizing these patterns over time can retain the temporal trends of ST series. We aim to propose an evolution pattern- aware and narrative- preserving visualization for large- scale ST series."
  },
  {
    "sentence": "The geographic information is hard to integrate.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1549,
    "para_id": 238,
    "paper_id": 4,
    "2d_coord": [
      -2.69087553024292,
      -1.0238463878631592
    ],
    "MSU_id": 1549,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Our design employs linked views. The Storyline is already compactly laid out and takes up many effective visual channels. Thus, the geographic information is hard to integrate."
  },
  {
    "sentence": "Tang et al. realized that a hand-drawn storyline is more aesthetically pleasing.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1555,
    "para_id": 239,
    "paper_id": 4,
    "2d_coord": [
      0.3980023264884949,
      -1.639542818069458
    ],
    "MSU_id": 1555,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Storylines have been applied to various scenarios, such as software analysis [39], compound event reviewing [32], fiction presentation [30], video moderation [53], and meeting recalling [44]. Initially, Storylines were generated by hand. To automate the generation, researcher developed many algorithms, such as, based on genetic algorithms [50] or hybrid optimization [30]. Tang et al. [52] realized that a hand- drawn storyline is more aesthetically pleasing and developed an authoring tool iStoryline with a new layout optimization framework. Most recently,"
  },
  {
    "sentence": "Tang et al. [51] improved iStoryline via reinforcement learning.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1557,
    "para_id": 240,
    "paper_id": 4,
    "2d_coord": [
      -1.8304082155227661,
      -1.6299264430999756
    ],
    "MSU_id": 1557,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Tang et al. [51] improved iStoryline via reinforcement learning."
  },
  {
    "sentence": "Yagi et al. made an initial attempt on a small dataset with a few ST series and short-term observations.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1561,
    "para_id": 241,
    "paper_id": 4,
    "2d_coord": [
      -2.7115085124969482,
      -1.4884591102600098
    ],
    "MSU_id": 1561,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "In the above studies, the relationships between entities are explicit. For example, two entities have a relationship if the characters of entities appear in the same scene [30]. By contrast, we need to model implicit relationships between large- scale ST series and construct sessions. Yagi et al. [62] made an initial attempt on a small dataset with a few ST series and short- term observations. Their approach ignored the spatial context and the temporal sensitivity of relationship modeling, and might require tedious processes with trial and error to tune session generation."
  },
  {
    "sentence": "Their approach ignored the spatial context and the temporal sensitivity of relationship modeling.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1562,
    "para_id": 241,
    "paper_id": 4,
    "2d_coord": [
      -2.9735889434814453,
      -1.2703437805175781
    ],
    "MSU_id": 1562,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "In the above studies, the relationships between entities are explicit. For example, two entities have a relationship if the characters of entities appear in the same scene [30]. By contrast, we need to model implicit relationships between large- scale ST series and construct sessions. Yagi et al. [62] made an initial attempt on a small dataset with a few ST series and short- term observations. Their approach ignored the spatial context and the temporal sensitivity of relationship modeling, and might require tedious processes with trial and error to tune session generation."
  },
  {
    "sentence": "Their approach might require tedious processes with trial and error to tune session generation.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1563,
    "para_id": 241,
    "paper_id": 4,
    "2d_coord": [
      -0.6286956667900085,
      -2.2083494663238525
    ],
    "MSU_id": 1563,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "In the above studies, the relationships between entities are explicit. For example, two entities have a relationship if the characters of entities appear in the same scene [30]. By contrast, we need to model implicit relationships between large- scale ST series and construct sessions. Yagi et al. [62] made an initial attempt on a small dataset with a few ST series and short- term observations. Their approach ignored the spatial context and the temporal sensitivity of relationship modeling, and might require tedious processes with trial and error to tune session generation."
  },
  {
    "sentence": "Such a manner is not scalable.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1566,
    "para_id": 242,
    "paper_id": 4,
    "2d_coord": [
      -2.1506495475769043,
      -1.492863655090332
    ],
    "MSU_id": 1566,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "ST series are also associated with the spatial context. Yagi et al. [62] used colors to visually link the curves in Storyline with the locations on a geographic map. Such a manner is not scalable. Hulstein et al. [17] summarized and compared different ways to combine the spatial context and Storyline. Although their study did not focus on large- scale data, their findings provide useful insights into effective visual designs."
  },
  {
    "sentence": "Although their study did not focus on large-scale data, their findings provide useful insights into effective visual designs.",
    "subtitle": "2 RELATED WORK",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 1568,
    "para_id": 242,
    "paper_id": 4,
    "2d_coord": [
      -0.050122398883104324,
      -1.7966862916946411
    ],
    "MSU_id": 1568,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "ST series are also associated with the spatial context. Yagi et al. [62] used colors to visually link the curves in Storyline with the locations on a geographic map. Such a manner is not scalable. Hulstein et al. [17] summarized and compared different ways to combine the spatial context and Storyline. Although their study did not focus on large- scale data, their findings provide useful insights into effective visual designs."
  },
  {
    "sentence": "Existing visualization methods are either unable to highlight meaningful knowledge or avoid the intractable seeking process of information in numerous ST series by analysts.",
    "subtitle": "3.2 Background and Research Problem",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1576,
    "para_id": 244,
    "paper_id": 4,
    "2d_coord": [
      0.7063496112823486,
      -1.3029810190200806
    ],
    "MSU_id": 1576,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "This study resulted from a long- term experience in spatiotemporal analysis. We have collaborated with many experts in environmental science, geography, and urban computing fields in various projects. We found that although many visual analytics methods have been proposed for ST series analysis [9, 10, 13, 23], there is still a lack of an intuitive and effective visualization to display large- scale ST series to provide the awareness or understanding of data. As mentioned in Sec. 2, ST series have two fundamental features to visualize, namely, the temporal trend and spatial context. When dealing with large- scale ST series, analysts have a vast geographic space to explore, and many long time series to browse. Existing visualization methods become limited. They are either unable to highlight meaningful knowledge and avoid the intractable seeking process of information in numerous ST series by analysts, or cannot clearly display the inherent temporal trends and dynamics of ST series. Instead of a complicated application, this paper studies a fundamental problem, i.e., effectively visualizing the temporal trends of large- scale ST series with a spatial context."
  },
  {
    "sentence": "Existing visualization methods cannot clearly display the inherent temporal trends and dynamics of ST series.",
    "subtitle": "3.2 Background and Research Problem",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1577,
    "para_id": 244,
    "paper_id": 4,
    "2d_coord": [
      0.04640045389533043,
      -0.8017150163650513
    ],
    "MSU_id": 1577,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "This study resulted from a long- term experience in spatiotemporal analysis. We have collaborated with many experts in environmental science, geography, and urban computing fields in various projects. We found that although many visual analytics methods have been proposed for ST series analysis [9, 10, 13, 23], there is still a lack of an intuitive and effective visualization to display large- scale ST series to provide the awareness or understanding of data. As mentioned in Sec. 2, ST series have two fundamental features to visualize, namely, the temporal trend and spatial context. When dealing with large- scale ST series, analysts have a vast geographic space to explore, and many long time series to browse. Existing visualization methods become limited. They are either unable to highlight meaningful knowledge and avoid the intractable seeking process of information in numerous ST series by analysts, or cannot clearly display the inherent temporal trends and dynamics of ST series. Instead of a complicated application, this paper studies a fundamental problem, i.e., effectively visualizing the temporal trends of large- scale ST series with a spatial context."
  },
  {
    "sentence": "Storyline occupies the visual channel of height, which is originally used to reveal the temporal trend in line charts.",
    "subtitle": "3.3 Storyline-based Solution",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1609,
    "para_id": 253,
    "paper_id": 4,
    "2d_coord": [
      0.5085976719856262,
      -1.1585006713867188
    ],
    "MSU_id": 1609,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Incorporating spatiotemporal visualizations. Evolution patterns are well organized in a narrative way by analogy to sessions in a Storyline. The second challenge is the limited space in the compact Storyline hinders the embedding of the spatiotemporal visualization of ST series. In addition, Storyline occupies the visual channel of height, which is originally used to reveal the temporal trend in line charts. To this end, GeoChron includes a novel two- level visualization mechanism based on the Storyline to encode the spatiotemporal information from overview to details."
  },
  {
    "sentence": "A smaller size is not enough to capture prominent features.",
    "subtitle": "4 PATTERN MINING",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1621,
    "para_id": 255,
    "paper_id": 4,
    "2d_coord": [
      -2.775693893432617,
      -0.7137378454208374
    ],
    "MSU_id": 1621,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Step 1: Slicing Time Span. The first step is to transform the continuous temporal dimension into discrete slices. After detecting sessions for each slice, we can apply Storyline. Our framework divides the time span evenly into  $T$  slices, denoted as  ${s_1,\\ldots ,s_T}$ , with equal size (e.g., Fig. 3A). Each time slice  $s = (t_s,t_e)$  has a time period structure. The slice size depends on the domain- specific scenario. For example, air pollution data are often reviewed on a daily basis [55], and the size of one day is appropriate. A smaller size is not enough to capture prominent features, and a larger size may miss the dynamics. Afterward, Steps 2- 4 below are executed for each time slice."
  },
  {
    "sentence": "A larger size may miss the dynamics.",
    "subtitle": "4 PATTERN MINING",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1622,
    "para_id": 255,
    "paper_id": 4,
    "2d_coord": [
      -2.8718767166137695,
      -0.7197009325027466
    ],
    "MSU_id": 1622,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Step 1: Slicing Time Span. The first step is to transform the continuous temporal dimension into discrete slices. After detecting sessions for each slice, we can apply Storyline. Our framework divides the time span evenly into  $T$  slices, denoted as  ${s_1,\\ldots ,s_T}$ , with equal size (e.g., Fig. 3A). Each time slice  $s = (t_s,t_e)$  has a time period structure. The slice size depends on the domain- specific scenario. For example, air pollution data are often reviewed on a daily basis [55], and the size of one day is appropriate. A smaller size is not enough to capture prominent features, and a larger size may miss the dynamics. Afterward, Steps 2- 4 below are executed for each time slice."
  },
  {
    "sentence": "The two aspects have distinct scales and semantics, making weight tuning difficult.",
    "subtitle": "4 PATTERN MINING",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1641,
    "para_id": 260,
    "paper_id": 4,
    "2d_coord": [
      -3.1931204795837402,
      -0.22711174190044403
    ],
    "MSU_id": 1641,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Such a network- based formulation fuses the temporal correlation and geographic context well. Assigning weights to each aspect and summing them to form a distance or similarity measure for clustering may be a straightforward approach. Yet, the two aspects have distinct scales and semantics, making weight tuning difficult, and the summation lacks semantic clarity. By contrast, our formulation preserves the semantics in time and space respectively to construct a relation network."
  },
  {
    "sentence": "The summation of weights lacks semantic clarity.",
    "subtitle": "4 PATTERN MINING",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1642,
    "para_id": 260,
    "paper_id": 4,
    "2d_coord": [
      -0.6419157981872559,
      -1.909130334854126
    ],
    "MSU_id": 1642,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Such a network- based formulation fuses the temporal correlation and geographic context well. Assigning weights to each aspect and summing them to form a distance or similarity measure for clustering may be a straightforward approach. Yet, the two aspects have distinct scales and semantics, making weight tuning difficult, and the summation lacks semantic clarity. By contrast, our formulation preserves the semantics in time and space respectively to construct a relation network."
  },
  {
    "sentence": "StoryFlow's positioning method based on quadratic programming usually runs out of memory for large-scale data.",
    "subtitle": "5 ORIGINAL STORYLINE LAYOUT METHOD",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1676,
    "para_id": 266,
    "paper_id": 4,
    "2d_coord": [
      -1.622999906539917,
      -1.6498847007751465
    ],
    "MSU_id": 1676,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Step 3: Positioning. Two adjacent entities in the same session should have a space  $d_{in}$  between each other, and two adjacent sessions should have a space  $d_{out}$  between each other (Fig. 4E).  $d_{out} > d_{in}$ . Aligned entities in the aligned sessions should have the same  $y$ - positions and can be rendered with straight lines. Sessions and entities should be compactly positioned under the constraints above. StoryFlow's positioning method based on the quadratic programming [30] usually runs out of memory for large- scale data [49]. Thus, we adopt a heuristic implementation of StoryFlow's positioning method to position entities by sweeping time slices back and forth."
  },
  {
    "sentence": "A geographic map is difficult to integrate into the compact and already information-dense Storyline.",
    "subtitle": "6.1 Design Goals",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1701,
    "para_id": 271,
    "paper_id": 4,
    "2d_coord": [
      -0.6061317920684814,
      -2.06213641166687
    ],
    "MSU_id": 1701,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "G1 Enable session- based analysis. Evolution patterns serve as entries for analyzing multiple ST series that are close and correlated. Since every pattern is represented as a session in the Storyline, the visualization should enable interactive analysis based on sessions. G2 Support effective tracking. The correlation relationship between ST series is dynamic and thereby evolution patterns present complex spatial and temporal relationships between each other. Therefore, the design should implement flexible interactions for tracking the evolution patterns as well as ST series in the Storyline. G3 Visualize temporal trends. Time series trends are the basic features in ST series [74]. Trends can facilitate the locating of interesting periods, e.g., when the readings of multiple ST series suddenly rise or fall. Besides, trends help users interpret dynamic correlation relations, e.g., why two ST series are correlated. Hence, the design should visually display the trends of time series over time. G4 Link spatial and temporal information. The spatial information of ST series is important for understanding evolution patterns. A geographic map is difficult to integrate into the compact and already information- dense Storyline. Therefore, linking the spatial and temporal information is more suitable than integrating them. G5 Provide multi- level analysis workflow. Large- scale ST series can exhibit rich information, e.g., the time series trends over a long time, spatial distribution on a vast space, and many- to- many spatiotemporal relations. Multi- level analysis is a well- known effective manner for accommodating a large amount of information. Thus, the design should support a multi- level analysis workflow."
  },
  {
    "sentence": "StoryFlow's solution also has the issue of consuming much vertical space.",
    "subtitle": "6.2 Layout Refinement",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1714,
    "para_id": 274,
    "paper_id": 4,
    "2d_coord": [
      -1.0695573091506958,
      -2.1845855712890625
    ],
    "MSU_id": 1714,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Loose Alignment. Feeding numerous sessions into the Storyline layout algorithm above may produce a hard- to- read representation. The sessions sparsely distribute vertically and users have to scroll a lot to browse them. The main reason is that when two aligned entities are straightened, the straightening process pushes other sessions up or down entirely over many time slices (Fig. 4A), consuming much vertical space. StoryFlow's solution [30] also has this issue. To alleviate this issue, we discard those session pairs whose number of entities in common is less than a threshold  $th_{c}$ . For example, compared with Fig. 4B1, Fig. 4B2 is more space- efficient since the curve pointed by the red arrow is not aligned."
  },
  {
    "sentence": "These methods can improve readability but also bring different degrees of information loss.",
    "subtitle": "6.2 Layout Refinement",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1723,
    "para_id": 276,
    "paper_id": 4,
    "2d_coord": [
      -0.4539106488227844,
      -2.3692784309387207
    ],
    "MSU_id": 1723,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "These methods can improve readability but also bring different degrees of information loss. Users need to make a trade- off between information completeness and readability on demand. Moreover, the readability should be perceived by users. For example, how much do the curves affect the visual continuity? How much do users have to scroll to browse patterns? Do the patterns overwhelm users? Therefore, GeoChron allows users to specify the above parameters via sliders."
  },
  {
    "sentence": "Due to a large number of involved dots, it is inappropriate to assign each dot a unique color like previous methods.",
    "subtitle": "6.3.1 Tracking Overall Evolution Patterns",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 1769,
    "para_id": 285,
    "paper_id": 4,
    "2d_coord": [
      -1.0178934335708618,
      -1.935202956199646
    ],
    "MSU_id": 1769,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Linking spatial context with the Storyline (G4). We implement a geographic map, where each ST series is plotted as a dot according to its geographic position (Fig. 5C). The map hovers above the Storyline. Users can interactively drag and drop it. Afterward, we need to link the dots on the map to the curves in the Storyline. Due to a large number of involved dots, it is inappropriate to assign each dot a unique color like previous methods [10, 62]. Moreover, our study focuses on evolution patterns (i.e., sessions) rather than individual ST series (i.e., entities). Thus, we design an interactive session- based coloring (G1)."
  },
  {
    "sentence": "The coloring process should not be automated.",
    "subtitle": "6.3.1 Tracking Overall Evolution Patterns",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 1790,
    "para_id": 289,
    "paper_id": 4,
    "2d_coord": [
      -1.44318425655365,
      -2.298518180847168
    ],
    "MSU_id": 1790,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "The coloring process is difficult to automate, nor should it be automated. There are innumerable coloring results with different visual patterns. We leave the process of visual pattern discovery to users because users know what their concerns are better than the machine."
  },
  {
    "sentence": "P2 and P6 commented that 'GeoChron is somehow misleading if ST series that are close in space are not placed together in the Storyline.'",
    "subtitle": "8.2 Informal User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2011,
    "para_id": 322,
    "paper_id": 4,
    "2d_coord": [
      -2.5838518142700195,
      -0.4691874384880066
    ],
    "MSU_id": 2011,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Fig. 8 summarizes the ICE- T scores.  $95\\%$  C.I. are  $6.41\\pm 0.26$  (I),  $5.45\\pm 0.60$  (C),  $6.58\\pm 0.22$  (E), and  $6.27\\pm 0.44$  (T). All participants agreed that GeoChron can provide insights, facilitate exploration, and reveal the essence of data, as the scores of Insight, Essence, and Time are about 6. The Confidence scores are relatively low, which is reasonable. We assume the dataset is cleaned and thus do not design visualizations for data quality. P2 and P6 also commented, \"GeoChron is somehow misleading if ST series that are close in space are not placed together in the Storyline.\" Their confusion was dispelled during the study after we clarified the encoding of the vertical position again."
  },
  {
    "sentence": "Users may have difficulty browsing the visualization when the layout is high.",
    "subtitle": "8.3 Ablation Study and Parameter Analysis",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2047,
    "para_id": 327,
    "paper_id": 4,
    "2d_coord": [
      -0.25698065757751465,
      -2.0891172885894775
    ],
    "MSU_id": 2047,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Afterward, we study how loose alignment and session filtering affect the layout of GeoChron via parameter analysis on  $th_s$  and  $th_c$ . We obtain different layouts generated under different combinations of  $th_s$  and  $th_c$ . Then, we use the average height of all time slices to quantify each layout's height (Fig. 10A). The layout is high when these two parameters are small (e.g., Fig. 10F). Users may have difficulty browsing the visualization. Increasing the two parameters can compact the layout. They work by different principles. On the one hand, in creasing  $th_c$  results in the layout with looser alignment (e.g., Fig. 10G). Users can still analyze those sessions with small sizes. On the other hand, increasing  $th_s$  results in the layout of Fig. 5A or D, where fewer sessions are displayed (Fig. 10B)."
  },
  {
    "sentence": "Users may have personalized analysis interests that are hard to quantify for automated optimization.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2083,
    "para_id": 333,
    "paper_id": 4,
    "2d_coord": [
      -0.8598619103431702,
      -0.9808342456817627
    ],
    "MSU_id": 2083,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Lessons. We have two lessons. First, interactive layout refining is useful. There are many possibilities for the visual layout of large- scale data. An ideal layout for the proposed visualization is hard to determine automatically. Users may have personalized analysis interests that are hard to quantify for automated optimization. In GeoChron, users are able to interactively refine the layout according to their needs, making a trade- off between information completeness and readability. In this way, the smoothness of layout adjustment needs to be guaranteed."
  },
  {
    "sentence": "To accommodate potential heterogeneity, the correlation modeling method should be replaced with one suitable for heterogeneous data.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2103,
    "para_id": 336,
    "paper_id": 4,
    "2d_coord": [
      0.34516575932502747,
      0.09314708411693573
    ],
    "MSU_id": 2103,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Generalizability. GeoChron can be applied to various kinds of data in addition to geo- spatial time series. First, visualizing cross- domain geo- spatial time series can reveal in- depth knowledge. For example, air quality is oftentimes correlated to temperature or traffic conditions [61]. To accommodate potential heterogeneity, the correlation modeling method should be replaced with one suitable for heterogeneous data."
  },
  {
    "sentence": "Our study has two limitations.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 2106,
    "para_id": 338,
    "paper_id": 4,
    "2d_coord": [
      -0.6525154113769531,
      -2.2683472633361816
    ],
    "MSU_id": 2106,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Limitations and Future Work. Our study has two limitations. First, GeoChron may take up a lot of horizontal space if there are too many time slices, requiring the user to scroll a lot horizontally. Although this issue can be potentially solved by increasing the size of slices, it may reduce the granularity and result in information loss. Besides, GeoChron may take up a lot of vertical space when the number of ST series increases. Although we allow users to refine and compress the layout, small but important patterns may be hidden in this way. In the future, we plan to study level- of- detail rendering to support larger scale data [8], for example, by abstracting the representation further [73]."
  },
  {
    "sentence": "GeoChron may take up a lot of horizontal space if there are too many time slices, requiring the user to scroll a lot horizontally.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2107,
    "para_id": 338,
    "paper_id": 4,
    "2d_coord": [
      0.10773272812366486,
      -1.722482681274414
    ],
    "MSU_id": 2107,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Limitations and Future Work. Our study has two limitations. First, GeoChron may take up a lot of horizontal space if there are too many time slices, requiring the user to scroll a lot horizontally. Although this issue can be potentially solved by increasing the size of slices, it may reduce the granularity and result in information loss. Besides, GeoChron may take up a lot of vertical space when the number of ST series increases. Although we allow users to refine and compress the layout, small but important patterns may be hidden in this way. In the future, we plan to study level- of- detail rendering to support larger scale data [8], for example, by abstracting the representation further [73]."
  },
  {
    "sentence": "Increasing the size of slices may reduce the granularity and result in information loss.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2108,
    "para_id": 338,
    "paper_id": 4,
    "2d_coord": [
      -0.6613581776618958,
      -2.4296152591705322
    ],
    "MSU_id": 2108,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Limitations and Future Work. Our study has two limitations. First, GeoChron may take up a lot of horizontal space if there are too many time slices, requiring the user to scroll a lot horizontally. Although this issue can be potentially solved by increasing the size of slices, it may reduce the granularity and result in information loss. Besides, GeoChron may take up a lot of vertical space when the number of ST series increases. Although we allow users to refine and compress the layout, small but important patterns may be hidden in this way. In the future, we plan to study level- of- detail rendering to support larger scale data [8], for example, by abstracting the representation further [73]."
  },
  {
    "sentence": "GeoChron may take up a lot of vertical space when the number of ST series increases.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2109,
    "para_id": 338,
    "paper_id": 4,
    "2d_coord": [
      -1.4345049858093262,
      0.24327218532562256
    ],
    "MSU_id": 2109,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Limitations and Future Work. Our study has two limitations. First, GeoChron may take up a lot of horizontal space if there are too many time slices, requiring the user to scroll a lot horizontally. Although this issue can be potentially solved by increasing the size of slices, it may reduce the granularity and result in information loss. Besides, GeoChron may take up a lot of vertical space when the number of ST series increases. Although we allow users to refine and compress the layout, small but important patterns may be hidden in this way. In the future, we plan to study level- of- detail rendering to support larger scale data [8], for example, by abstracting the representation further [73]."
  },
  {
    "sentence": "Refining and compressing the layout may hide small but important patterns.",
    "subtitle": "9 DISCUSSION AND CONCLUSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2110,
    "para_id": 338,
    "paper_id": 4,
    "2d_coord": [
      -0.7106990814208984,
      -2.3301260471343994
    ],
    "MSU_id": 2110,
    "paper_info": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "paragraph_info": "Limitations and Future Work. Our study has two limitations. First, GeoChron may take up a lot of horizontal space if there are too many time slices, requiring the user to scroll a lot horizontally. Although this issue can be potentially solved by increasing the size of slices, it may reduce the granularity and result in information loss. Besides, GeoChron may take up a lot of vertical space when the number of ST series increases. Although we allow users to refine and compress the layout, small but important patterns may be hidden in this way. In the future, we plan to study level- of- detail rendering to support larger scale data [8], for example, by abstracting the representation further [73]."
  },
  {
    "sentence": "The STC is plagued by well-known issues such as visual occlusion and depth ambiguity.",
    "subtitle": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2131,
    "para_id": 342,
    "paper_id": 3,
    "2d_coord": [
      -2.004913330078125,
      -2.039302349090576
    ],
    "MSU_id": 2131,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Abstract- Spatial time series visualization offers scientific research pathways and analytical decision- making tools across various spatiotemporal domains. Despite many advanced methodologies, the seamless integration of temporal and spatial information remains a challenge. The space- time cube (STC) stands out as a promising approach for the synergistic presentation of spatial and temporal information, with successful applications across various spatiotemporal datasets. However, the STC is plagued by well- known issues such as visual occlusion and depth ambiguity, which are further exacerbated when dealing with large- scale spatial time series data. In this study, we introduce a novel technical framework termed VolumeSTCube, designed for continuous spatiotemporal phenomena. It first leverages the concept of the STC to transform discretely distributed spatial time series data into continuously volumetric data. Subsequently, volume rendering and surface rendering techniques are employed to visualize the transformed volumetric data. Volume rendering is utilized to mitigate visual occlusion, while surface rendering provides pattern details by enhanced lighting information. Lastly, we design interactions to facilitate the exploration and analysis from temporal, spatial, and spatiotemporal perspectives. VolumeSTCube is evaluated through a computational experiment, a real- world case study with one expert, and a controlled user study with twelve non- experts, compared against a baseline from prior work, showing its superiority and effectiveness in large- scale spatial time series analysis."
  },
  {
    "sentence": "These issues are further exacerbated when dealing with large-scale spatial time series data.",
    "subtitle": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2132,
    "para_id": 342,
    "paper_id": 3,
    "2d_coord": [
      -0.9947701692581177,
      -1.6002603769302368
    ],
    "MSU_id": 2132,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Abstract- Spatial time series visualization offers scientific research pathways and analytical decision- making tools across various spatiotemporal domains. Despite many advanced methodologies, the seamless integration of temporal and spatial information remains a challenge. The space- time cube (STC) stands out as a promising approach for the synergistic presentation of spatial and temporal information, with successful applications across various spatiotemporal datasets. However, the STC is plagued by well- known issues such as visual occlusion and depth ambiguity, which are further exacerbated when dealing with large- scale spatial time series data. In this study, we introduce a novel technical framework termed VolumeSTCube, designed for continuous spatiotemporal phenomena. It first leverages the concept of the STC to transform discretely distributed spatial time series data into continuously volumetric data. Subsequently, volume rendering and surface rendering techniques are employed to visualize the transformed volumetric data. Volume rendering is utilized to mitigate visual occlusion, while surface rendering provides pattern details by enhanced lighting information. Lastly, we design interactions to facilitate the exploration and analysis from temporal, spatial, and spatiotemporal perspectives. VolumeSTCube is evaluated through a computational experiment, a real- world case study with one expert, and a controlled user study with twelve non- experts, compared against a baseline from prior work, showing its superiority and effectiveness in large- scale spatial time series analysis."
  },
  {
    "sentence": "Linked views require users to pay context-switching costs to browse the spatial and temporal visualizations separately.",
    "subtitle": "I. INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2157,
    "para_id": 345,
    "paper_id": 3,
    "2d_coord": [
      -0.5287594199180603,
      -2.489828586578369
    ],
    "MSU_id": 2157,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The effective composition of spatial and temporal visualizations has long been the most challenging problem in the visualization of spatiotemporal data beyond ST series [4], [5]. Linked views require users to pay context- switching costs to browse the spatial and temporal visualizations separately. In contrast, an integrated view tightly organizes the spatial and temporal visualizations to reduce such costs. The spacetime cube (STC) [6] is a kind of integrated view in a three- dimensional space, in which the space naturally occupies two dimensions (i.e., x- y plane), and the time constitutes the third dimension (i.e., z- axis). The position of the visual graphics can be directly related to both space and time dimensions. It is easy for users to accept and learn, and thus, users can perceive time and space simultaneously [7]. Moreover, in the STC, the evolution of spatiotemporal phenomena can be effectively conveyed through temporal narratives, as geographic- related information is visualized along a continuous timeline [8]."
  },
  {
    "sentence": "Adopting the STC to visualize large-scale ST series is still in an exploratory stage.",
    "subtitle": "I. INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2167,
    "para_id": 346,
    "paper_id": 3,
    "2d_coord": [
      -1.8481593132019043,
      -1.1034804582595825
    ],
    "MSU_id": 2167,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The STC works well for various datasets, such as spatiotemporal events [9]- [11] and trajectories [12], [13]. Yet, adopting the STC to visualize large- scale ST series is still in an exploratory stage. Prior studies [14]- [16] adopted the STC to visualize ST series (e.g., Figure 1), but only a few timestamps (e.g.,  $\\leq 30$  are applicable. It remains unexplored how to enable in- depth visual analysis of large- scale, highly dynamic, and multi- correlated ST series data based on the STC. To this end, this study aims to enhance the STC for visualizing large- scale ST series. However, two major challenges are posed."
  },
  {
    "sentence": "Prior studies adopted the STC to visualize ST series but only a few timestamps are applicable.",
    "subtitle": "I. INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2168,
    "para_id": 346,
    "paper_id": 3,
    "2d_coord": [
      -1.7572962045669556,
      -0.7957465648651123
    ],
    "MSU_id": 2168,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The STC works well for various datasets, such as spatiotemporal events [9]- [11] and trajectories [12], [13]. Yet, adopting the STC to visualize large- scale ST series is still in an exploratory stage. Prior studies [14]- [16] adopted the STC to visualize ST series (e.g., Figure 1), but only a few timestamps (e.g.,  $\\leq 30$  are applicable. It remains unexplored how to enable in- depth visual analysis of large- scale, highly dynamic, and multi- correlated ST series data based on the STC. To this end, this study aims to enhance the STC for visualizing large- scale ST series. However, two major challenges are posed."
  },
  {
    "sentence": "Thakur and Hanson's method causes the graphic plot of each ST series to occlude each other.",
    "subtitle": "I. INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2174,
    "para_id": 347,
    "paper_id": 3,
    "2d_coord": [
      -1.7501819133758545,
      -0.8554592132568359
    ],
    "MSU_id": 2174,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Effective presentation of spatiotemporal patterns. Due to the characteristics of the STC, each reading value of the ST series can directly correspond to time and space. However, the effective presentation of massive values is non- trivial. With Thakur and Hanson's method [14], the graphic plot of each ST series tends to occlude each other (Figure 1). Besides, the dataset's hidden spatiotemporal patterns (e.g., hotspots and propagation processes) cannot be revealed clearly. New data transformation and visualization strategies are strongly required to summarize and present the large- scale ST series in less visual occlusion."
  },
  {
    "sentence": "The dataset's hidden spatiotemporal patterns, such as hotspots and propagation processes, cannot be revealed clearly.",
    "subtitle": "I. INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2175,
    "para_id": 347,
    "paper_id": 3,
    "2d_coord": [
      -1.467777967453003,
      -2.120746374130249
    ],
    "MSU_id": 2175,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Effective presentation of spatiotemporal patterns. Due to the characteristics of the STC, each reading value of the ST series can directly correspond to time and space. However, the effective presentation of massive values is non- trivial. With Thakur and Hanson's method [14], the graphic plot of each ST series tends to occlude each other (Figure 1). Besides, the dataset's hidden spatiotemporal patterns (e.g., hotspots and propagation processes) cannot be revealed clearly. New data transformation and visualization strategies are strongly required to summarize and present the large- scale ST series in less visual occlusion."
  },
  {
    "sentence": "The depth channel exacerbates the issue of spatial correspondence.",
    "subtitle": "I. INTRODUCTION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2182,
    "para_id": 349,
    "paper_id": 3,
    "2d_coord": [
      -1.9962784051895142,
      -2.042407989501953
    ],
    "MSU_id": 2182,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "be far away from the map, and it may be difficult for users to associate the graphics with the geographical context. The depth channel also exacerbates the issue of spatial correspondence. It is cumbersome to pinpoint the desired selection in threedimensional space because the projection line of the object to the screen may intersect with multiple targets. Therefore, flexible interactions should be designed and implemented to accommodate the STC environment with large- scale ST series."
  },
  {
    "sentence": "Visual analysis that keeps humans in the loop is an important approach.",
    "subtitle": "A. Spatiotemporal Analysis",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2216,
    "para_id": 354,
    "paper_id": 3,
    "2d_coord": [
      0.9098924994468689,
      -1.0700981616973877
    ],
    "MSU_id": 2216,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Spatiotemporal analyses are commonly seen in various domains. Although many automated algorithms for spatiotemporal analysis have been proposed, the visual analysis that keeps humans in the loop is an important approach [17]. Below are some examples of ST series analysis, which is the focus of our study. Deng et al. [18] developed a tailored dynamic graph visualization to elucidate the dynamic causal relationships within ST series, facilitating spatially and temporally aware interpretation and validation. Li et al. [19] designed a visualization framework that enables the interactive extraction and exploration of event co- occurrences across ST series."
  },
  {
    "sentence": "These methods are limited in scalability.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2241,
    "para_id": 359,
    "paper_id": 3,
    "2d_coord": [
      -2.0405433177948,
      -1.4692800045013428
    ],
    "MSU_id": 2241,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "These methods are limited in scalability. Users need to make many spatial and time selections to browse a largescale ST series dataset that has wide spatial coverage and a large time span. Pre- summarizing or mining patterns within ST series before visualization, rather than directly depicting ST series, can lead to greater scalability but may result in information loss and disrupt the temporal narrative. Recently, Deng et al. [31] extracted evolution patterns from the ST series and organized them into narrative- preserving Storyline layout, proposing a scalable visualization technique."
  },
  {
    "sentence": "Users need to make many spatial and time selections to browse a large-scale ST series dataset that has wide spatial coverage and a large time span.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2242,
    "para_id": 359,
    "paper_id": 3,
    "2d_coord": [
      -1.170249342918396,
      -1.040264368057251
    ],
    "MSU_id": 2242,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "These methods are limited in scalability. Users need to make many spatial and time selections to browse a largescale ST series dataset that has wide spatial coverage and a large time span. Pre- summarizing or mining patterns within ST series before visualization, rather than directly depicting ST series, can lead to greater scalability but may result in information loss and disrupt the temporal narrative. Recently, Deng et al. [31] extracted evolution patterns from the ST series and organized them into narrative- preserving Storyline layout, proposing a scalable visualization technique."
  },
  {
    "sentence": "Pre-summarizing or mining patterns within ST series before visualization may result in information loss and disrupt the temporal narrative.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2244,
    "para_id": 359,
    "paper_id": 3,
    "2d_coord": [
      0.025536980479955673,
      -1.6606769561767578
    ],
    "MSU_id": 2244,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "These methods are limited in scalability. Users need to make many spatial and time selections to browse a largescale ST series dataset that has wide spatial coverage and a large time span. Pre- summarizing or mining patterns within ST series before visualization, rather than directly depicting ST series, can lead to greater scalability but may result in information loss and disrupt the temporal narrative. Recently, Deng et al. [31] extracted evolution patterns from the ST series and organized them into narrative- preserving Storyline layout, proposing a scalable visualization technique."
  },
  {
    "sentence": "This approach minimizes context switching costs.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2256,
    "para_id": 362,
    "paper_id": 3,
    "2d_coord": [
      -1.9818838834762573,
      -1.2198110818862915
    ],
    "MSU_id": 2256,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Integrated view. Integrated views are designed to tightly present spatial and temporal information, allowing users to analyze spatiotemporal data in one view without switching contexts. For example, Sun et al. [4] embedded traffic time series visualizations into road segments on the map after the roads are topologically expanded. Li et al. [34] designed a layout where timeline- based time series visualizations extend radially from the corresponding locations on the map. The glyph is also useful for integrating heterogeneous information [35], [36]. For example, in Deng et al's glyph [37], the temporal occurrence of a propagation pattern was wrapped on a circular map where the pattern is displayed. While this approach minimizes context switching costs, it employs distinct visual channels to encode temporal and spatial information separately, thereby limiting its intuitiveness."
  },
  {
    "sentence": "This approach employs distinct visual channels to encode temporal and spatial information separately.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2257,
    "para_id": 362,
    "paper_id": 3,
    "2d_coord": [
      -2.4532458782196045,
      -2.033001661300659
    ],
    "MSU_id": 2257,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Integrated view. Integrated views are designed to tightly present spatial and temporal information, allowing users to analyze spatiotemporal data in one view without switching contexts. For example, Sun et al. [4] embedded traffic time series visualizations into road segments on the map after the roads are topologically expanded. Li et al. [34] designed a layout where timeline- based time series visualizations extend radially from the corresponding locations on the map. The glyph is also useful for integrating heterogeneous information [35], [36]. For example, in Deng et al's glyph [37], the temporal occurrence of a propagation pattern was wrapped on a circular map where the pattern is displayed. While this approach minimizes context switching costs, it employs distinct visual channels to encode temporal and spatial information separately, thereby limiting its intuitiveness."
  },
  {
    "sentence": "Employing distinct visual channels to encode temporal and spatial information separately limits its intuitiveness.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2258,
    "para_id": 362,
    "paper_id": 3,
    "2d_coord": [
      -2.4939544200897217,
      -1.9687774181365967
    ],
    "MSU_id": 2258,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Integrated view. Integrated views are designed to tightly present spatial and temporal information, allowing users to analyze spatiotemporal data in one view without switching contexts. For example, Sun et al. [4] embedded traffic time series visualizations into road segments on the map after the roads are topologically expanded. Li et al. [34] designed a layout where timeline- based time series visualizations extend radially from the corresponding locations on the map. The glyph is also useful for integrating heterogeneous information [35], [36]. For example, in Deng et al's glyph [37], the temporal occurrence of a propagation pattern was wrapped on a circular map where the pattern is displayed. While this approach minimizes context switching costs, it employs distinct visual channels to encode temporal and spatial information separately, thereby limiting its intuitiveness."
  },
  {
    "sentence": "The STC-based visualization of ST series is less explored compared to linked views.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2260,
    "para_id": 363,
    "paper_id": 3,
    "2d_coord": [
      -0.2777881622314453,
      -0.22690264880657196
    ],
    "MSU_id": 2260,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The space- time cube (STC) can be considered as an integrated view. The STC- based visualization of ST series is less explored compared to linked views. Existing studies [14]- [16] primarily represent each ST series as a column within the space- time cube, positioning each column according to its geographical location. While STC- based visualizations provide an integrated presentation of spatial and temporal information, they face scalability issues when dealing with large- scale ST series, as will be discussed in the next subsection."
  },
  {
    "sentence": "STC-based visualizations face scalability issues when dealing with large-scale ST series.",
    "subtitle": "B. Spatial Time (ST) Series Visualization",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2264,
    "para_id": 363,
    "paper_id": 3,
    "2d_coord": [
      -0.05959874764084816,
      -0.5303879976272583
    ],
    "MSU_id": 2264,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The space- time cube (STC) can be considered as an integrated view. The STC- based visualization of ST series is less explored compared to linked views. Existing studies [14]- [16] primarily represent each ST series as a column within the space- time cube, positioning each column according to its geographical location. While STC- based visualizations provide an integrated presentation of spatial and temporal information, they face scalability issues when dealing with large- scale ST series, as will be discussed in the next subsection."
  },
  {
    "sentence": "The STC sets itself apart from 2D visualization techniques.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2271,
    "para_id": 365,
    "paper_id": 3,
    "2d_coord": [
      -0.15202081203460693,
      -2.1820483207702637
    ],
    "MSU_id": 2271,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The space- time cube (STC) was originally proposed by T. Hagerstrand in the early 70s [38] to depict the life histories of humans. The STC seamlessly integrates spatial and temporal dimensions within a 3D cube space, comprising a geographic map (i.e., a plane) and a vertical time axis. Such a design enables users to view the entire spatiotemporal dataset in a single view with both spatial context and temporal information concurrently. This sets it apart from 2D visualization techniques, which typically require users to navigate slider controls for temporal exploration or toggle between a geographic map and a separate time- oriented visualization."
  },
  {
    "sentence": "2D visualization techniques typically require users to navigate slider controls for temporal exploration or toggle between a geographic map and a separate time-oriented visualization.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2272,
    "para_id": 365,
    "paper_id": 3,
    "2d_coord": [
      0.25327444076538086,
      -1.7372864484786987
    ],
    "MSU_id": 2272,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "The space- time cube (STC) was originally proposed by T. Hagerstrand in the early 70s [38] to depict the life histories of humans. The STC seamlessly integrates spatial and temporal dimensions within a 3D cube space, comprising a geographic map (i.e., a plane) and a vertical time axis. Such a design enables users to view the entire spatiotemporal dataset in a single view with both spatial context and temporal information concurrently. This sets it apart from 2D visualization techniques, which typically require users to navigate slider controls for temporal exploration or toggle between a geographic map and a separate time- oriented visualization."
  },
  {
    "sentence": "The STC has the same well-known limitation as other 3D data representations: occlusion.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2275,
    "para_id": 366,
    "paper_id": 3,
    "2d_coord": [
      -3.104940891265869,
      -1.3391087055206299
    ],
    "MSU_id": 2275,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "STC applications. The STC has been successfully applied in the visualization of spatiotemporal datasets in various domains, such as earthquake events [9], crime clusters [10], mobilities [39], and eye- tracking recordings [40]. However, the STC has the same well- known limitation as other 3D data representations: occlusion [41]. For example, if there are more time series and more timestamps, there will be occlusion between columns (Figure 1). The user may suffer from the process of searching the patterns from many multiple columns that are separately distributed in the cube."
  },
  {
    "sentence": "If there are more time series and more timestamps, there will be occlusion between columns.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2276,
    "para_id": 366,
    "paper_id": 3,
    "2d_coord": [
      -2.032503128051758,
      -1.6587400436401367
    ],
    "MSU_id": 2276,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "STC applications. The STC has been successfully applied in the visualization of spatiotemporal datasets in various domains, such as earthquake events [9], crime clusters [10], mobilities [39], and eye- tracking recordings [40]. However, the STC has the same well- known limitation as other 3D data representations: occlusion [41]. For example, if there are more time series and more timestamps, there will be occlusion between columns (Figure 1). The user may suffer from the process of searching the patterns from many multiple columns that are separately distributed in the cube."
  },
  {
    "sentence": "These visualizations focus on summarizing the video without facilitating interactive exploration of the details.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2284,
    "para_id": 368,
    "paper_id": 3,
    "2d_coord": [
      -0.0580756850540638,
      -1.9732093811035156
    ],
    "MSU_id": 2284,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Volume visualization. In the earliest applications, the integration of volume data and space- time cubes (STC) was prevalent in video visualizations, as the sequence of frames in a video inherently constitutes volumetric data. Video visualiza tions aim to efficiently capture activities within videos [42]- [45] and identify areas of interest for viewers [46]. These visualizations focus on summarizing the video without facilitating interactive exploration of the details, whereas ST series capture many local but important patterns."
  },
  {
    "sentence": "These studies have been conducted on small datasets.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2290,
    "para_id": 369,
    "paper_id": 3,
    "2d_coord": [
      -2.3007750511169434,
      -1.91605544090271
    ],
    "MSU_id": 2290,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Such a combination was also adopted in the geographic domain. Demsar and Virrantaus [47] transformed trajectories in STC into volumetric density representations using 3D kernel density estimation (KDE). Similarly, Nakaya and Yano [10] represented crime clusters as volumetric densities in STC via 3D KDE. The visualizations generated through volume rendering can mitigate occlusion issues by revealing the overall distribution characteristics of data in a 3D space. These studies, however, have been conducted on small datasets, leaving the interactive exploratory approach for large- scale ST series unexplored."
  },
  {
    "sentence": "A few interactions were designed to explore STC with large-scale spatiotemporal data.",
    "subtitle": "C.Space-Time Cube",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2294,
    "para_id": 370,
    "paper_id": 3,
    "2d_coord": [
      -2.131709575653076,
      -1.6570788621902466
    ],
    "MSU_id": 2294,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Interactions in STC. Elaborate user interactions, such as probing [48], rotation [49], and cutting [50] can be applied to ease visual occlusion and enhance usability via flexible 3D exploration. Bach et al. [51] reviewed the existing STC interactions and presented a descriptive framework for interacting with a generalized space- time cube. However, a few interactions were designed to explore STC with large- scale spatiotemporal data. Filho et al. [39] introduced a set of interactions for selecting and filtering large- scale taxi trips in the STC from spatial and temporal perspectives. Our study aims to propose STC interactions for ST series of spatiotemporal phenomena. Specifically, we follow Bach et al.'s framework [51] and design STC interactions from temporal, spatial, and spatiotemporal perspectives to assist users in exploring large- scale ST series."
  },
  {
    "sentence": "Although many ST series visualizations are designed to present temporal trends with the spatial context, they require users to make costly switches between geographic and temporal information.",
    "subtitle": "B. Research Problem and Solution",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2316,
    "para_id": 375,
    "paper_id": 3,
    "2d_coord": [
      -1.048393964767456,
      -1.7431960105895996
    ],
    "MSU_id": 2316,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "With the development of sensor technology and the decrease in storage costs, more and more fine- grained large- scale ST series are collected. To analyze large- scale data adequately, analysts need effective and interactive visualization techniques. However, as concluded in section II, although many ST series visualizations are designed to present temporal trends with the spatial context, they require users to make costly switches between geographic and temporal information, which is particularly unfriendly in the face of large- scale datasets. The spacetime cube is the most promising technique for alleviating the context switching cost. Yet, existing attempts focused on small- scale datasets and may experience occlusion problems (Figure 1). Our research problem is refined to address how to adapt the space- time cube to large- scale ST series."
  },
  {
    "sentence": "This requirement is particularly unfriendly in the face of large-scale datasets.",
    "subtitle": "B. Research Problem and Solution",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2317,
    "para_id": 375,
    "paper_id": 3,
    "2d_coord": [
      0.18928490579128265,
      -1.2199695110321045
    ],
    "MSU_id": 2317,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "With the development of sensor technology and the decrease in storage costs, more and more fine- grained large- scale ST series are collected. To analyze large- scale data adequately, analysts need effective and interactive visualization techniques. However, as concluded in section II, although many ST series visualizations are designed to present temporal trends with the spatial context, they require users to make costly switches between geographic and temporal information, which is particularly unfriendly in the face of large- scale datasets. The spacetime cube is the most promising technique for alleviating the context switching cost. Yet, existing attempts focused on small- scale datasets and may experience occlusion problems (Figure 1). Our research problem is refined to address how to adapt the space- time cube to large- scale ST series."
  },
  {
    "sentence": "Existing attempts focused on small-scale datasets and may experience occlusion problems.",
    "subtitle": "B. Research Problem and Solution",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2319,
    "para_id": 375,
    "paper_id": 3,
    "2d_coord": [
      -2.363431215286255,
      -2.055741310119629
    ],
    "MSU_id": 2319,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "With the development of sensor technology and the decrease in storage costs, more and more fine- grained large- scale ST series are collected. To analyze large- scale data adequately, analysts need effective and interactive visualization techniques. However, as concluded in section II, although many ST series visualizations are designed to present temporal trends with the spatial context, they require users to make costly switches between geographic and temporal information, which is particularly unfriendly in the face of large- scale datasets. The spacetime cube is the most promising technique for alleviating the context switching cost. Yet, existing attempts focused on small- scale datasets and may experience occlusion problems (Figure 1). Our research problem is refined to address how to adapt the space- time cube to large- scale ST series."
  },
  {
    "sentence": "The smoothness of the 2D scalar field can be affected by poor sensor quality, unstable data transmission, and inherent issues.",
    "subtitle": "B. Transformation",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2361,
    "para_id": 382,
    "paper_id": 3,
    "2d_coord": [
      -1.5474227666854858,
      -1.5653756856918335
    ],
    "MSU_id": 2361,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "1) Interpolation: First, we divide the geographic space into an  $n\\times m$  grid, resulting in  $n\\times m$  cells denoted as  $C = {c_{1,1},c_{1,2},\\ldots ,c_{n,m}}$ . Each cell is of uniform size and sufficiently small. For example, the geographic space in Figure 2A is divided into a  $3\\times 3$  grid with 9 cells. Second, we divide the ST series with  $T$  timestamps by their timestamps into  $T$  slices. For the  $t$ -th slice of the  $t$  timestamp,  $t\\leq T$ , there are observed data samples  $Z_{t} = {z_{1,t},z_{2,t},\\ldots ,z_{ST}}$ , with  $z_{i,t} = v_{i,t}$  representing the value of the  $i$ -th series at timestamp  $t$ . Afterwards, interpolation techniques, like Kriging and Inverse Distance Weighted, can be employed to predict  $\\hat{z}{c{x,y}}$  for each grid  $c_{x,y}\\in C$  based on observed data samples  $Z_{t}$ , generating a sample  $(s = (x,y),t,v = \\hat{z}{c{x,y}})$  in the spacetime volume. For each slice of each timestamp, we obtain a 2D scalar field with  $n\\times m$  cells. After performing the above operations for every slice, we stack the 2D scalar fields to form a completed space-time volume.\n2) Smoothing: Each generated 2D scalar field is smooth over the geographic space because of the spatial autocorrelation principle. However, due to many reasons, e.g., poor sensor quality, unstable data transmission, and the inherent"
  },
  {
    "sentence": "Efficient interactions do not depend on context-free UI elements such as input fields and sliders.",
    "subtitle": "D. Interactions",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2423,
    "para_id": 396,
    "paper_id": 3,
    "2d_coord": [
      -0.4396951198577881,
      -2.1084866523742676
    ],
    "MSU_id": 2423,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "1) Considerations: We reviewed linked views and integrated views in the 2D display and derived three kinds of interactions for spatiotemporal visualization. Notably, the efficient ones focus on direct manipulation of spatial or temporal visualizations, instead of depending on context-free UI elements such as input fields and sliders. Thus, we considered the tight integration of visualization and interactions in the interaction design."
  },
  {
    "sentence": "A useful method is to integrate another 2D view in the 3D space for brushing.",
    "subtitle": "D. Interactions",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 2428,
    "para_id": 397,
    "paper_id": 3,
    "2d_coord": [
      -0.39213016629219055,
      -1.518894910812378
    ],
    "MSU_id": 2428,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "First, users usually drill down into a time range of interest for analyzing a dataset with long- term observations. They can specify the time range based on the focus+context mechanism [58]- [60] or directly based on prior knowledge [61]. Brushing on the visualization along the timeline aligns with user interaction practices in conventional 2D views. A useful method is to integrate another 2D view in the 3D space for brushing [39]. In contrast, we map the brushing interaction to the volume slicing along the timeline (Figure 4A), allowing users to directly perform selection in the 3D space with less context switching. Similarly, given a vast space where data is hard to analyze, users usually perform spatial range selection first [3], [62]. Again, the selections with lassos and polygons on the map are difficult to issue in the 3D space. Hence, we design an interaction called volume spotlight for users to select a spatial range in the cube space (Figure 4B)."
  },
  {
    "sentence": "Selections with lassos and polygons on the map are difficult to issue in the 3D space.",
    "subtitle": "D. Interactions",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 2431,
    "para_id": 397,
    "paper_id": 3,
    "2d_coord": [
      -1.8446224927902222,
      -1.1664828062057495
    ],
    "MSU_id": 2431,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "First, users usually drill down into a time range of interest for analyzing a dataset with long- term observations. They can specify the time range based on the focus+context mechanism [58]- [60] or directly based on prior knowledge [61]. Brushing on the visualization along the timeline aligns with user interaction practices in conventional 2D views. A useful method is to integrate another 2D view in the 3D space for brushing [39]. In contrast, we map the brushing interaction to the volume slicing along the timeline (Figure 4A), allowing users to directly perform selection in the 3D space with less context switching. Similarly, given a vast space where data is hard to analyze, users usually perform spatial range selection first [3], [62]. Again, the selections with lassos and polygons on the map are difficult to issue in the 3D space. Hence, we design an interaction called volume spotlight for users to select a spatial range in the cube space (Figure 4B)."
  },
  {
    "sentence": "Running in a browser may not meet the memory resource needs for volume rendering.",
    "subtitle": "E. Implementation",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2483,
    "para_id": 409,
    "paper_id": 3,
    "2d_coord": [
      0.09069947898387909,
      0.4012775123119354
    ],
    "MSU_id": 2483,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "VolumeSTCube is a desktop application. We choose Unity as our development platform over Three.js based on WebGL. This decision considers that running in a browser may not meet the memory resource needs for volume rendering due to different browsers' memory management restrictions. The development environment is a desktop running Windows 10 with an Intel Core i7- 13700K 3.40GHz CPU, NVIDIA GeForce RTX 3070 8GB GPU, and 32 GB of RAM."
  },
  {
    "sentence": "Different browsers have memory management restrictions.",
    "subtitle": "E. Implementation",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2484,
    "para_id": 409,
    "paper_id": 3,
    "2d_coord": [
      -2.4648165702819824,
      0.06693106889724731
    ],
    "MSU_id": 2484,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "VolumeSTCube is a desktop application. We choose Unity as our development platform over Three.js based on WebGL. This decision considers that running in a browser may not meet the memory resource needs for volume rendering due to different browsers' memory management restrictions. The development environment is a desktop running Windows 10 with an Intel Core i7- 13700K 3.40GHz CPU, NVIDIA GeForce RTX 3070 8GB GPU, and 32 GB of RAM."
  },
  {
    "sentence": "These approaches differ fundamentally in design and interaction principles from STC-based methods.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2630,
    "para_id": 433,
    "paper_id": 3,
    "2d_coord": [
      -1.657791018486023,
      -1.2522594928741455
    ],
    "MSU_id": 2630,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "We conducted a controlled, within- subject user study. The study primarily aimed to verify 1) the effectiveness of continuous volume- based visualization compared to column- based visualization and 2) the ease of understanding of volume- based visualization combined with the space- time cube. Besides, we hoped to identify the strengths and weaknesses of VolumeSTCube. While numerous methods, such as 2D map- based estimations or coordinated 2D maps and line charts, are available for comparison, these approaches differ fundamentally in design and interaction principles from STC- based methods. We selected Thakur and Hanson's STC- based method [14] as the baseline, shown in Figure 1. This method, the most recent STC- based visualization for ST series, lacks volumetric representation but serves as a representative of similar methods [15], [16]. Its design allows a focused evaluation of the benefits introduced by volume- based visualization."
  },
  {
    "sentence": "Thakur and Hanson's method lacks volumetric representation but serves as a representative of similar methods.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2633,
    "para_id": 433,
    "paper_id": 3,
    "2d_coord": [
      -3.160207509994507,
      -0.6451888084411621
    ],
    "MSU_id": 2633,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "We conducted a controlled, within- subject user study. The study primarily aimed to verify 1) the effectiveness of continuous volume- based visualization compared to column- based visualization and 2) the ease of understanding of volume- based visualization combined with the space- time cube. Besides, we hoped to identify the strengths and weaknesses of VolumeSTCube. While numerous methods, such as 2D map- based estimations or coordinated 2D maps and line charts, are available for comparison, these approaches differ fundamentally in design and interaction principles from STC- based methods. We selected Thakur and Hanson's STC- based method [14] as the baseline, shown in Figure 1. This method, the most recent STC- based visualization for ST series, lacks volumetric representation but serves as a representative of similar methods [15], [16]. Its design allows a focused evaluation of the benefits introduced by volume- based visualization."
  },
  {
    "sentence": "In the baseline, visualizations were influenced by station density.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2723,
    "para_id": 446,
    "paper_id": 3,
    "2d_coord": [
      -1.167452096939087,
      -0.3545963764190674
    ],
    "MSU_id": 2723,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q7, Q8, and Q13. These three questions required the subjects to examine the value distribution across the geographic space at a given timestamp. To do that, the slicing interaction was helpful regardless of the system used. Consequently, the response times were typically under 20 seconds, with most being under 10 seconds. However, in the baseline, visualizations were influenced by station density, leading some subjects to mistakenly perceive areas with dense disk density as more polluted, resulting in an accuracy of only  $66.7\\%$  of Q7 and Q8."
  },
  {
    "sentence": "Some subjects mistakenly perceived areas with dense disk density as more polluted.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2724,
    "para_id": 446,
    "paper_id": 3,
    "2d_coord": [
      -0.8083803653717041,
      -0.4113353192806244
    ],
    "MSU_id": 2724,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q7, Q8, and Q13. These three questions required the subjects to examine the value distribution across the geographic space at a given timestamp. To do that, the slicing interaction was helpful regardless of the system used. Consequently, the response times were typically under 20 seconds, with most being under 10 seconds. However, in the baseline, visualizations were influenced by station density, leading some subjects to mistakenly perceive areas with dense disk density as more polluted, resulting in an accuracy of only  $66.7\\%$  of Q7 and Q8."
  },
  {
    "sentence": "The baseline demands subjects repeatedly select time ranges and columns to mitigate occlusion caused by columns.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2730,
    "para_id": 448,
    "paper_id": 3,
    "2d_coord": [
      -2.527036190032959,
      -2.0241739749908447
    ],
    "MSU_id": 2730,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q11. To answer Q11, subjects are required to explore all timestamps across the entire geographic area. Using VolumeSTCube, subjects can examine all voxels without encountering occlusion issues and easily identify the reddest and most opaque ones through simple rotations (only a few seconds). In contrast, the baseline demands subjects repeatedly select time ranges and columns to mitigate occlusion caused by columns, thereby prolonging interaction times."
  },
  {
    "sentence": "The baseline approach prolongs interaction times.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2731,
    "para_id": 448,
    "paper_id": 3,
    "2d_coord": [
      -0.9939688444137573,
      -1.3734707832336426
    ],
    "MSU_id": 2731,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q11. To answer Q11, subjects are required to explore all timestamps across the entire geographic area. Using VolumeSTCube, subjects can examine all voxels without encountering occlusion issues and easily identify the reddest and most opaque ones through simple rotations (only a few seconds). In contrast, the baseline demands subjects repeatedly select time ranges and columns to mitigate occlusion caused by columns, thereby prolonging interaction times."
  },
  {
    "sentence": "Subjects using the baseline method found it challenging to compare hotspots due to occlusion.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2735,
    "para_id": 449,
    "paper_id": 3,
    "2d_coord": [
      -2.6798157691955566,
      -1.2046740055084229
    ],
    "MSU_id": 2735,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q12. To solve Q12, subjects should identify the largest hotspot. In VolumeSTCube, identifying hotspots is expedited as they are presented as distinct voxel clusters. Subjects can easily compare the size and color of each voxel cluster, and thereby identify the largest one. Conversely, subjects using the baseline method found it challenging to compare hotspots due to occlusion, especially in densely clustered columns in North China, as shown in Figure 1. They resorted to selecting time ranges and columns to estimate the largest hotspot, which was time- consuming and imprecise. Consequently, the baseline takes 8 seconds longer on average than VolumeSTCube. Furthermore, with the baseline method, one subject gave up, and two subjects provided incorrect answers, resulting in an accuracy rate of  $75\\%$ , which is lower than VolumeSTCube's accuracy rate of  $100\\%$ ."
  },
  {
    "sentence": "Occlusion was especially problematic in densely clustered columns in North China, as shown in Figure 1.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 2736,
    "para_id": 449,
    "paper_id": 3,
    "2d_coord": [
      -1.3011806011199951,
      -1.8358314037322998
    ],
    "MSU_id": 2736,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q12. To solve Q12, subjects should identify the largest hotspot. In VolumeSTCube, identifying hotspots is expedited as they are presented as distinct voxel clusters. Subjects can easily compare the size and color of each voxel cluster, and thereby identify the largest one. Conversely, subjects using the baseline method found it challenging to compare hotspots due to occlusion, especially in densely clustered columns in North China, as shown in Figure 1. They resorted to selecting time ranges and columns to estimate the largest hotspot, which was time- consuming and imprecise. Consequently, the baseline takes 8 seconds longer on average than VolumeSTCube. Furthermore, with the baseline method, one subject gave up, and two subjects provided incorrect answers, resulting in an accuracy rate of  $75\\%$ , which is lower than VolumeSTCube's accuracy rate of  $100\\%$ ."
  },
  {
    "sentence": "Subjects using the baseline method resorted to selecting time ranges and columns to estimate the largest hotspot.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2737,
    "para_id": 449,
    "paper_id": 3,
    "2d_coord": [
      -1.9492158889770508,
      -1.4430439472198486
    ],
    "MSU_id": 2737,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q12. To solve Q12, subjects should identify the largest hotspot. In VolumeSTCube, identifying hotspots is expedited as they are presented as distinct voxel clusters. Subjects can easily compare the size and color of each voxel cluster, and thereby identify the largest one. Conversely, subjects using the baseline method found it challenging to compare hotspots due to occlusion, especially in densely clustered columns in North China, as shown in Figure 1. They resorted to selecting time ranges and columns to estimate the largest hotspot, which was time- consuming and imprecise. Consequently, the baseline takes 8 seconds longer on average than VolumeSTCube. Furthermore, with the baseline method, one subject gave up, and two subjects provided incorrect answers, resulting in an accuracy rate of  $75\\%$ , which is lower than VolumeSTCube's accuracy rate of  $100\\%$ ."
  },
  {
    "sentence": "Estimating the largest hotspot using the baseline method was time-consuming and imprecise.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2738,
    "para_id": 449,
    "paper_id": 3,
    "2d_coord": [
      -2.860665798187256,
      -1.278546690940857
    ],
    "MSU_id": 2738,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q12. To solve Q12, subjects should identify the largest hotspot. In VolumeSTCube, identifying hotspots is expedited as they are presented as distinct voxel clusters. Subjects can easily compare the size and color of each voxel cluster, and thereby identify the largest one. Conversely, subjects using the baseline method found it challenging to compare hotspots due to occlusion, especially in densely clustered columns in North China, as shown in Figure 1. They resorted to selecting time ranges and columns to estimate the largest hotspot, which was time- consuming and imprecise. Consequently, the baseline takes 8 seconds longer on average than VolumeSTCube. Furthermore, with the baseline method, one subject gave up, and two subjects provided incorrect answers, resulting in an accuracy rate of  $75\\%$ , which is lower than VolumeSTCube's accuracy rate of  $100\\%$ ."
  },
  {
    "sentence": "In the baseline, the propagation process is hidden in multiple discretely distributed columns.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2742,
    "para_id": 450,
    "paper_id": 3,
    "2d_coord": [
      -0.9943581819534302,
      -2.3560938835144043
    ],
    "MSU_id": 2742,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q17. In the baseline, the propagation process is hidden in multiple discretely distributed columns and subjects had to move the time range back and forth and observe changes in the color or radius of every disk, which is time- consuming with the mental burden. Nonetheless, subjects using VolumeSTCube can easily identify a propagation process via the rendered surface in a continuous form (e.g., shown in Figure 7) and select two cities based on the propagation path. As a result, VolumeSTCube outperformed the baseline by over 40 seconds regarding response time; two subjects gave up when using the baseline, resulting in a correctness of  $66.7\\%$ ."
  },
  {
    "sentence": "Subjects had to move the time range back and forth and observe changes in the color or radius of every disk, which is time-consuming with the mental burden.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2743,
    "para_id": 450,
    "paper_id": 3,
    "2d_coord": [
      -1.1997867822647095,
      -1.27872896194458
    ],
    "MSU_id": 2743,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Q17. In the baseline, the propagation process is hidden in multiple discretely distributed columns and subjects had to move the time range back and forth and observe changes in the color or radius of every disk, which is time- consuming with the mental burden. Nonetheless, subjects using VolumeSTCube can easily identify a propagation process via the rendered surface in a continuous form (e.g., shown in Figure 7) and select two cities based on the propagation path. As a result, VolumeSTCube outperformed the baseline by over 40 seconds regarding response time; two subjects gave up when using the baseline, resulting in a correctness of  $66.7\\%$ ."
  },
  {
    "sentence": "Two subjects noted that the opacity of voxels could potentially hinder analyzing low pollution levels.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2767,
    "para_id": 454,
    "paper_id": 3,
    "2d_coord": [
      0.21498946845531464,
      -0.9542405605316162
    ],
    "MSU_id": 2767,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Visualization. All subjects quickly grasped the space- time cube and confirmed the intuitiveness of VolumeSTCube.Moreover, all subjects using VolumeSTCube for analysis experienced minimal occlusion problems and remarked that \"due to the opacity of voxels (actually the volume visualization they meant), they experienced no visual occlusion during analysis.\" Two subjects noted that the opacity of voxels could potentially hinder analyzing low pollution levels, which corresponds to VolumeSTCube's poor performance in Q15. In the future, we plan to implement a rendering parameter adjustment control to alleviate this problem. For example, the mapping of values to transparency and color can be adjusted interactively. Finally, most subjects expressed that surface rendering offered clear boundaries for voxel clusters, aiding in obtaining specific values at spatiotemporal positions and assessing propagation processes. This capability could be difficult to achieve solely with volume rendering. Interestingly, some subjects with strong spatial perception felt relying solely on volume visualization was sufficient."
  },
  {
    "sentence": "The opacity of voxels corresponds to VolumeSTCube's poor performance in Q15.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2768,
    "para_id": 454,
    "paper_id": 3,
    "2d_coord": [
      -0.8846518993377686,
      -0.6284927725791931
    ],
    "MSU_id": 2768,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Visualization. All subjects quickly grasped the space- time cube and confirmed the intuitiveness of VolumeSTCube.Moreover, all subjects using VolumeSTCube for analysis experienced minimal occlusion problems and remarked that \"due to the opacity of voxels (actually the volume visualization they meant), they experienced no visual occlusion during analysis.\" Two subjects noted that the opacity of voxels could potentially hinder analyzing low pollution levels, which corresponds to VolumeSTCube's poor performance in Q15. In the future, we plan to implement a rendering parameter adjustment control to alleviate this problem. For example, the mapping of values to transparency and color can be adjusted interactively. Finally, most subjects expressed that surface rendering offered clear boundaries for voxel clusters, aiding in obtaining specific values at spatiotemporal positions and assessing propagation processes. This capability could be difficult to achieve solely with volume rendering. Interestingly, some subjects with strong spatial perception felt relying solely on volume visualization was sufficient."
  },
  {
    "sentence": "This capability could be difficult to achieve solely with volume rendering.",
    "subtitle": "B. User Study",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2773,
    "para_id": 454,
    "paper_id": 3,
    "2d_coord": [
      -0.6815967559814453,
      -1.9021720886230469
    ],
    "MSU_id": 2773,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "Visualization. All subjects quickly grasped the space- time cube and confirmed the intuitiveness of VolumeSTCube.Moreover, all subjects using VolumeSTCube for analysis experienced minimal occlusion problems and remarked that \"due to the opacity of voxels (actually the volume visualization they meant), they experienced no visual occlusion during analysis.\" Two subjects noted that the opacity of voxels could potentially hinder analyzing low pollution levels, which corresponds to VolumeSTCube's poor performance in Q15. In the future, we plan to implement a rendering parameter adjustment control to alleviate this problem. For example, the mapping of values to transparency and color can be adjusted interactively. Finally, most subjects expressed that surface rendering offered clear boundaries for voxel clusters, aiding in obtaining specific values at spatiotemporal positions and assessing propagation processes. This capability could be difficult to achieve solely with volume rendering. Interestingly, some subjects with strong spatial perception felt relying solely on volume visualization was sufficient."
  },
  {
    "sentence": "This section discusses the limitations of VolumeSTCube.",
    "subtitle": "VI. DISCUSSION",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2781,
    "para_id": 456,
    "paper_id": 3,
    "2d_coord": [
      -0.201736718416214,
      -2.1122329235076904
    ],
    "MSU_id": 2781,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "This section discusses the implications, generalizability, and limitations of VolumeSTCube, and posts the future work."
  },
  {
    "sentence": "VolumeSTCube is not suitable for ST series with discrete spatial distributions.",
    "subtitle": "B. Generalizability",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2801,
    "para_id": 459,
    "paper_id": 3,
    "2d_coord": [
      0.3036918044090271,
      -0.5683720111846924
    ],
    "MSU_id": 2801,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "VolumeSTCube is not suitable for ST series with discrete spatial distributions, such as tourist visits to different attractions or demographic trends across countries. For example, a country located between two populous countries may have a significantly smaller population. Due to their spatially discrete nature, converting these types of ST series into continuous volumetric data would be ineffective and misleading. Topological analysis that extracts spatially discrete events can be an effective alternative for supporting visual exploration of these datasets [67]."
  },
  {
    "sentence": "Converting ST series with spatially discrete nature into continuous volumetric data would be ineffective and misleading.",
    "subtitle": "B. Generalizability",
    "category": "LIMITATION",
    "rank": 5,
    "msuid": 2804,
    "para_id": 459,
    "paper_id": 3,
    "2d_coord": [
      -0.7137678861618042,
      0.1555120050907135
    ],
    "MSU_id": 2804,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "VolumeSTCube is not suitable for ST series with discrete spatial distributions, such as tourist visits to different attractions or demographic trends across countries. For example, a country located between two populous countries may have a significantly smaller population. Due to their spatially discrete nature, converting these types of ST series into continuous volumetric data would be ineffective and misleading. Topological analysis that extracts spatially discrete events can be an effective alternative for supporting visual exploration of these datasets [67]."
  },
  {
    "sentence": "The scalability of VolumeSTCube is primarily constrained by the hardware's capacity to handle a certain number of voxels.",
    "subtitle": "C. Scalability",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2806,
    "para_id": 460,
    "paper_id": 3,
    "2d_coord": [
      -2.0767178535461426,
      -1.380302906036377
    ],
    "MSU_id": 2806,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "C. ScalabilityThe scalability of VolumeSTCube is primarily constrained by the hardware's capacity to handle a certain number of voxels. Under the hardware conditions mentioned in subsection IV-E, VolumeSTCube can effectively visualize ST series spanning half a year across China, with a spatial granularity of a  $350 \\times 350$  grid and a temporal granularity of hours, totaling over 50 million voxels. In cases where hardware resources are limited, adjustments such as reducing the number of cells or aggregating the time can be implemented to mitigate the number of voxels while maintaining the representation of ST series within the same range."
  },
  {
    "sentence": "There is a lack of a rendering parameter control mentioned in subsubsection V-B3.",
    "subtitle": "D. Limitations and Future Work",
    "category": "LIMITATION",
    "rank": 4,
    "msuid": 2818,
    "para_id": 462,
    "paper_id": 3,
    "2d_coord": [
      -1.4998499155044556,
      -1.6120643615722656
    ],
    "MSU_id": 2818,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "In addition to the lack of a rendering parameter control mentioned in subsubsection V- B3, we also identify the following limitations or future work."
  },
  {
    "sentence": "We identify additional limitations or future work.",
    "subtitle": "D. Limitations and Future Work",
    "category": "LIMITATION",
    "rank": 3,
    "msuid": 2819,
    "para_id": 462,
    "paper_id": 3,
    "2d_coord": [
      -0.6621208786964417,
      -2.3741517066955566
    ],
    "MSU_id": 2819,
    "paper_info": "Visualizing Large-Scale Spatial Time Series with GeoChron",
    "paragraph_info": "In addition to the lack of a rendering parameter control mentioned in subsubsection V- B3, we also identify the following limitations or future work."
  }
]